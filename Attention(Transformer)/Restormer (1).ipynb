{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "pYQQHoD-RwG-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MDTA(nn.Module):\n",
    "    '''***IMPORTANT*** - The channels must be zero when divided by num_heads'''\n",
    "    def __init__(self, channels, num_heads): \n",
    "        super(MDTA, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
    "        \n",
    "        # *3 for q, k, v & chunk(3, dim=1)\n",
    "        # 1x1 Conv to aggregate pixel-wise cross-channel context \n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)  \n",
    "        # 3x3 DWConv to encode channel-wise spatial context\n",
    "        self.qkv_conv = nn.Conv2d(channels * 3, channels * 3, kernel_size=3, padding=1, groups=channels * 3, bias=False)\n",
    "        # 1x1 Point-wise Conv\n",
    "        self.project_out = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''(N, C, H, W) -> (N, C, H, W)\n",
    "        Output of MDTA feature should be added to input feature x'''\n",
    "        b, c, h, w = x.shape\n",
    "        q, k, v = self.qkv_conv(self.qkv(x)).chunk(3, dim=1)  # (N, C, H, W)\n",
    "        \n",
    "        # divide the # of channels into heads & learn separate attention map\n",
    "        q = q.reshape(b, self.num_heads, -1, h * w)  # (N, num_heads, C/num_heads, HW)\n",
    "        k = k.reshape(b, self.num_heads, -1, h * w)\n",
    "        v = v.reshape(b, self.num_heads, -1, h * w)\n",
    "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1)\n",
    "        \n",
    "        # CxC Attention map instead of HWxHW (when num_heads=1)\n",
    "        attn = torch.softmax(torch.matmul(q, k.transpose(-2, -1).contiguous()) * self.temperature, dim=-1)  # (N, num_heads, C/num_heads, C/num_heads)\n",
    "        out = self.project_out(torch.matmul(attn, v).reshape(b, -1, h, w))  # attn*v: (N, num_heads, C/num_heads, HW)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class GDFN(nn.Module):\n",
    "    def __init__(self, channels, expansion_factor):\n",
    "        super(GDFN, self).__init__()\n",
    "\n",
    "        hidden_channels = int(channels * expansion_factor)  # channel expansion \n",
    "        # 1x1 conv to extend feature channel\n",
    "        self.project_in = nn.Conv2d(channels, hidden_channels * 2, kernel_size=1, bias=False)\n",
    "\n",
    "        # 3x3 DConv (groups=input_channels) -> each input channel is convolved with its own set of filters\n",
    "        self.conv = nn.Conv2d(hidden_channels * 2, hidden_channels * 2, kernel_size=3, padding=1,\n",
    "                              groups=hidden_channels * 2, bias=False)\n",
    "        \n",
    "        # 1x1 conv to reduce channels back to original input dimension\n",
    "        self.project_out = nn.Conv2d(hidden_channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''HxWxC -> HxWxC\n",
    "        Output of GDFN feature should be added to input feature x'''\n",
    "        x1, x2 = self.conv(self.project_in(x)).chunk(2, dim=1)\n",
    "        # Gating: the element-wise product of 2 parallel paths of linear transformation layers \n",
    "        x = self.project_out(F.gelu(x1) * x2)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    '''***IMPORTANT*** - The channels must be zero when divided by num_heads'''\n",
    "    def __init__(self, channels, num_heads, expansion_factor):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        assert channels % num_heads == 0\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.attn = MDTA(channels, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.ffn = GDFN(channels, expansion_factor)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''(N, C, H, W) -> (N, C, H, W)'''\n",
    "        b, c, h, w = x.shape        \n",
    "        # Add MDTA output feature\n",
    "        x = x + self.attn(self.norm1(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                          .contiguous().reshape(b, c, h, w))\n",
    "        # ADD GDFN output feature\n",
    "        x = x + self.ffn(self.norm2(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                         .contiguous().reshape(b, c, h, w))\n",
    "        return x\n",
    "\n",
    "\n",
    "class DownSample(nn.Module):\n",
    "    '''Channel x 2, Resolution x 1/2 by PixelUnshuffle'''\n",
    "    def __init__(self, channels):\n",
    "        super(DownSample, self).__init__()\n",
    "        self.body = nn.Sequential(nn.Conv2d(channels, channels // 2, kernel_size=3, padding=1, bias=False),\n",
    "                                  nn.PixelUnshuffle(2))  # (N, C, H, W) -> (N, 4C, H/2, W/2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''(N, C, H, W) -> (N, 2C, H/2, W/2)'''\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    '''Channel x 1/2, Resolution x 2 by PixelShuffle'''\n",
    "    def __init__(self, channels):\n",
    "        super(UpSample, self).__init__()\n",
    "        self.body = nn.Sequential(nn.Conv2d(channels, channels * 2, kernel_size=3, padding=1, bias=False),\n",
    "                                  nn.PixelShuffle(2))  # (N, C, H, W) -> (N, C/4, 2H, 2W)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''(N, C, H, W) -> (N, C/2, 2H, 2W)'''\n",
    "        return self.body(x)\n",
    "\n",
    "\n",
    "class Restormer(nn.Module):\n",
    "    def __init__(self, num_blocks=[4, 6, 6, 8], num_heads=[1, 2, 4, 8], channels=[48, 96, 192, 384], num_refinement=4,\n",
    "                 expansion_factor=2.66):\n",
    "        super(Restormer, self).__init__()\n",
    "\n",
    "        self.embed_conv = nn.Conv2d(3, 48, kernel_size=3, padding=1, bias=False)\n",
    "        \n",
    "        self.encoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(\n",
    "            num_ch, num_ah, expansion_factor) for _ in range(num_tb)]) for num_tb, num_ah, num_ch in\n",
    "                                       zip(num_blocks, num_heads, channels)])\n",
    "        # TransformerBlock(48, 1, r) x 4\n",
    "        # TransformerBlock(96, 2, r) x 6\n",
    "        # TransformerBlock(192, 4, r) x 6\n",
    "        # TransformerBlock(384, 8, r) x 8\n",
    "        \n",
    "        # the number of down sample or up sample == the number of encoder - 1\n",
    "        self.downs = nn.ModuleList([DownSample(num_ch) for num_ch in channels[:-1]])\n",
    "        # DownSample(48)\n",
    "        # DownSample(96)\n",
    "        # DownSample(192)\n",
    "        # DownSample(384)\n",
    "\n",
    "        self.ups = nn.ModuleList([UpSample(num_ch) for num_ch in list(reversed(channels))[:-1]])\n",
    "        # UpSample(384)\n",
    "        # UpSample(192)\n",
    "        # UpSample(96)\n",
    "\n",
    "        # the number of reduce block == the number of decoder - 1\n",
    "        self.reduces = nn.ModuleList([nn.Conv2d(channels[i], channels[i - 1], kernel_size=1, bias=False)\n",
    "                                      for i in reversed(range(2, len(channels)))])\n",
    "        # Conv2d(384, 192)\n",
    "        # Conv2d(192, 96)\n",
    "        \n",
    "        # the number of decoder == the number of encoder - 1\n",
    "        self.decoders = nn.ModuleList([nn.Sequential(*[TransformerBlock(channels[2], num_heads[2], expansion_factor)\n",
    "                                                       for _ in range(num_blocks[2])])])\n",
    "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[1], expansion_factor)\n",
    "                                             for _ in range(num_blocks[1])]))\n",
    "        # the channel of last one is not change\n",
    "        self.decoders.append(nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
    "                                             for _ in range(num_blocks[0])]))\n",
    "        # TransformerBlock(192, 4, r) x 6\n",
    "        # TransformerBlock(96, 2, r) x 6\n",
    "        # TransformerBlock(96, 1, r) x 4\n",
    "\n",
    "        self.refinement = nn.Sequential(*[TransformerBlock(channels[1], num_heads[0], expansion_factor)\n",
    "                                          for _ in range(num_refinement)])\n",
    "        # TransformerBlock(96, 2, r) x 4\n",
    "\n",
    "        self.output = nn.Conv2d(96, 3, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):  # (N, 3, H, W)\n",
    "\n",
    "        # low-level feature\n",
    "        fo = self.embed_conv(x)   # (N, 48, H, W)\n",
    "\n",
    "        out_enc1 = self.encoders[0](fo)  # (N, 48, H, W)\n",
    "        out_enc2 = self.encoders[1](self.downs[0](out_enc1))  # (N, 96, H/2, W/2) -> (N, 96, H/2, W/2)\n",
    "        out_enc3 = self.encoders[2](self.downs[1](out_enc2))  # (N, 192, H/4, W/4) -> (N, 192, H/4, W/4)\n",
    "        out_enc4 = self.encoders[3](self.downs[2](out_enc3))  # (N, 384, H/8, W/8) -> (N, 384, H/8, W/8)\n",
    "        \n",
    "        # aggregate(concatenate) low-level feature of encoder with the high-level feature of decoder\n",
    "        out_dec3 = self.decoders[0](self.reduces[0](torch.cat([self.ups[0](out_enc4), out_enc3], dim=1)))  # (N, 192, H/4, W/4) -> (N, 192, H/4, W/4)\n",
    "        out_dec2 = self.decoders[1](self.reduces[1](torch.cat([self.ups[1](out_dec3), out_enc2], dim=1)))  # (N, 96, H/2, W/2) -> (N, 96, H/2, W/2)\n",
    "\n",
    "        # deep feature\n",
    "        fd = self.decoders[2](torch.cat([self.ups[2](out_dec2), out_enc1], dim=1))  # (N, 96, H, W)\n",
    "        \n",
    "        # refinement at high resolution\n",
    "        fr = self.refinement(fd)  # (N, 96, H, W)\n",
    "\n",
    "        # restored_img = residual_img + degraded_img\n",
    "        out = self.output(fr) + x  #  (N, 3, H, W)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T5Ukws-ERyIi",
    "outputId": "cbf7f842-f6d1-4037-8296-fedf1a141de9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn([2, 3, 64, 64]).cuda()\n",
    "model = Restormer().cuda()\n",
    "out = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7pBc4kc7R3YX",
    "outputId": "a77eed56-f735-4166-db65-fb4bdc18ce50"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEQEMGAxSSPr"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
