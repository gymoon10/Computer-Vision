{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e45b57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.model_zoo as modelzoo\n",
    "\n",
    "backbone_url = 'https://github.com/CoinCheung/BiSeNet/releases/download/0.0.0/backbone_v2.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db606297",
   "metadata": {},
   "source": [
    "## 0. Basic Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54afb36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chan, out_chan, ks=3, stride=1, padding=1,\n",
    "                 dilation=1, groups=1, bias=False):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "        self.conv = nn.Conv2d(\n",
    "                in_chan, out_chan, kernel_size=ks, stride=stride,\n",
    "                padding=padding, dilation=dilation,\n",
    "                groups=groups, bias=bias)\n",
    "        self.bn = nn.BatchNorm2d(out_chan)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.conv(x)\n",
    "        feat = self.bn(feat)\n",
    "        feat = self.relu(feat)     \n",
    "        return feat\n",
    "    \n",
    "class UpSample(nn.Module):\n",
    "\n",
    "    def __init__(self, n_chan, factor=2):\n",
    "        super(UpSample, self).__init__()\n",
    "        out_chan = n_chan * factor * factor\n",
    "        self.proj = nn.Conv2d(n_chan, out_chan, 1, 1, 0)\n",
    "        self.up = nn.PixelShuffle(factor)\n",
    "        self.init_weight()\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.proj(x)  # (N, out_chan, h, 2)\n",
    "        feat = self.up(feat)  # (N, out_chan/factor^2, 2*h, 2*w)\n",
    "        return feat  # (N, n_chan, 2*h, 2*w)\n",
    "\n",
    "    def init_weight(self):\n",
    "        nn.init.xavier_normal_(self.proj.weight, gain=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239aa109",
   "metadata": {},
   "source": [
    "## 1. Detail Branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0680de20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetailBranch(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(DetailBranch, self).__init__()\n",
    "        self.S1 = nn.Sequential(\n",
    "            ConvBNReLU(3, 64, 3, stride=2),\n",
    "            ConvBNReLU(64, 64, 3, stride=1),\n",
    "        )\n",
    "        self.S2 = nn.Sequential(\n",
    "            ConvBNReLU(64, 64, 3, stride=2),\n",
    "            ConvBNReLU(64, 64, 3, stride=1),\n",
    "            ConvBNReLU(64, 64, 3, stride=1),\n",
    "        )\n",
    "        self.S3 = nn.Sequential(\n",
    "            ConvBNReLU(64, 128, 3, stride=2),\n",
    "            ConvBNReLU(128, 128, 3, stride=1),\n",
    "            ConvBNReLU(128, 128, 3, stride=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.S1(x)  # (N, 64, h/2, w/2)\n",
    "        feat = self.S2(feat)  # (N, 64, h/4, w/4)\n",
    "        feat = self.S3(feat)  # (N, 128, h/8, w/8)\n",
    "        return feat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af923d",
   "metadata": {},
   "source": [
    "## 2. Semantic Branch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e8b1ba",
   "metadata": {},
   "source": [
    "### 2.1 Stem Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40eb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StemBlock(nn.Module):\n",
    "    '''1st stage of semantic branch\n",
    "    - Block with 2 branches with different manners \n",
    "    to downsample the feature representation'''\n",
    "    def __init__(self):\n",
    "        super(StemBlock, self).__init__()\n",
    "        self.conv = ConvBNReLU(3, 16, 3, stride=2)  # shared\n",
    "        \n",
    "        # conv path\n",
    "        self.left = nn.Sequential(\n",
    "            ConvBNReLU(16, 8, 1, stride=1, padding=0),\n",
    "            ConvBNReLU(8, 16, 3, stride=2),\n",
    "        )\n",
    "        \n",
    "        # max-pool path\n",
    "        self.right = nn.MaxPool2d(\n",
    "            kernel_size=3, stride=2, padding=1, ceil_mode=False)\n",
    "        \n",
    "        # fuse\n",
    "        self.fuse = ConvBNReLU(32, 16, 3, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.conv(x)  # (N, 16, 2/h, w/2)\n",
    "        \n",
    "        feat_left = self.left(feat)  # (N, 16, h/4, w/4)\n",
    "        feat_right = self.right(feat)  # (N, 16, h/4, w/4)\n",
    "        \n",
    "        feat = torch.cat([feat_left, feat_right], dim=1)  # (N, 32, h/4, w/4)\n",
    "        feat = self.fuse(feat)  # (N, 16, h/4, w/4)\n",
    "        \n",
    "        return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae6a3e9",
   "metadata": {},
   "source": [
    "### 2.2 Context Embedding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5c5a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEBlock(nn.Module):\n",
    "    '''last stage of semantic branch \n",
    "    for context embedding'''\n",
    "    def __init__(self):\n",
    "        super(CEBlock, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(128)\n",
    "        self.conv_gap = ConvBNReLU(128, 128, 1, stride=1, padding=0)\n",
    "        #TODO: in paper here is naive conv2d, no bn-relu\n",
    "        self.conv_last = ConvBNReLU(128, 128, 3, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = torch.mean(x, dim=(2, 3), keepdim=True)\n",
    "        feat = self.bn(feat)\n",
    "        feat = self.conv_gap(feat)\n",
    "        feat = feat + x\n",
    "        feat = self.conv_last(feat)\n",
    "        \n",
    "        return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf2ed7c",
   "metadata": {},
   "source": [
    "### 2.3 Gather and Expansion Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4cfec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELayerS1(nn.Module):\n",
    "    '''Gather & Expansion to aggregate feature responses and\n",
    "    expand to a high dimensional space'''\n",
    "    def __init__(self, in_chan, out_chan, exp_ratio=6):\n",
    "        super(GELayerS1, self).__init__()\n",
    "        mid_chan = in_chan * exp_ratio\n",
    "        # 3x3 conv\n",
    "        self.conv1 = ConvBNReLU(in_chan, in_chan, 3, stride=1)\n",
    "        \n",
    "        # depth-wise conv (expansion)\n",
    "        self.dwconv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_chan, mid_chan, kernel_size=3, stride=1,\n",
    "                padding=1, groups=in_chan, bias=False),\n",
    "            nn.BatchNorm2d(mid_chan),\n",
    "            nn.ReLU(inplace=True), # not shown in paper\n",
    "        )\n",
    "        \n",
    "        # 1x1 conv (reduction)\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                mid_chan, out_chan, kernel_size=1, stride=1,\n",
    "                padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_chan),\n",
    "        )\n",
    "        self.conv2[1].last_bn = True\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.conv1(x)\n",
    "        feat = self.dwconv(feat)\n",
    "        feat = self.conv2(feat)\n",
    "        feat = feat + x  # residual connection\n",
    "        feat = self.relu(feat)\n",
    "        return feat\n",
    "\n",
    "\n",
    "class GELayerS2(nn.Module):\n",
    "    '''Gather & Expansion for the stride=2 case'''\n",
    "    def __init__(self, in_chan, out_chan, exp_ratio=6):\n",
    "        super(GELayerS2, self).__init__()\n",
    "        mid_chan = in_chan * exp_ratio\n",
    "        # 3x3 conv\n",
    "        self.conv1 = ConvBNReLU(in_chan, in_chan, 3, stride=1)\n",
    "        \n",
    "        # depth-wise conv (expansion), stride=2\n",
    "        self.dwconv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_chan, mid_chan, kernel_size=3, stride=2,\n",
    "                padding=1, groups=in_chan, bias=False),\n",
    "            nn.BatchNorm2d(mid_chan),\n",
    "        )\n",
    "        \n",
    "        # depth-wise conv\n",
    "        self.dwconv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                mid_chan, mid_chan, kernel_size=3, stride=1,\n",
    "                padding=1, groups=mid_chan, bias=False),\n",
    "            nn.BatchNorm2d(mid_chan),\n",
    "            nn.ReLU(inplace=True), # not shown in paper\n",
    "        )\n",
    "        \n",
    "        # 1x1 conv\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                mid_chan, out_chan, kernel_size=1, stride=1,\n",
    "                padding=0, bias=False),\n",
    "            nn.BatchNorm2d(out_chan),\n",
    "        )\n",
    "        self.conv2[1].last_bn = True\n",
    "        \n",
    "        # separable path for residual connection\n",
    "        self.shortcut = nn.Sequential(\n",
    "                # 3x3 conv with stride=2\n",
    "                nn.Conv2d(\n",
    "                    in_chan, in_chan, kernel_size=3, stride=2,\n",
    "                    padding=1, groups=in_chan, bias=False),\n",
    "                nn.BatchNorm2d(in_chan),\n",
    "            \n",
    "                # 1x1 conv\n",
    "                nn.Conv2d(\n",
    "                    in_chan, out_chan, kernel_size=1, stride=1,\n",
    "                    padding=0, bias=False),\n",
    "                nn.BatchNorm2d(out_chan),\n",
    "        )\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self.conv1(x)\n",
    "        feat = self.dwconv1(feat)\n",
    "        feat = self.dwconv2(feat)\n",
    "        feat = self.conv2(feat)\n",
    "        \n",
    "        shortcut = self.shortcut(x)\n",
    "        \n",
    "        feat = feat + shortcut\n",
    "        feat = self.relu(feat)\n",
    "        \n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b14cd1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentBranch(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SegmentBranch, self).__init__()\n",
    "        self.S1S2 = StemBlock()\n",
    "        self.S3 = nn.Sequential(\n",
    "            GELayerS2(16, 32),\n",
    "            GELayerS1(32, 32),\n",
    "        )\n",
    "        self.S4 = nn.Sequential(\n",
    "            GELayerS2(32, 64),\n",
    "            GELayerS1(64, 64),\n",
    "        )\n",
    "        self.S5_4 = nn.Sequential(\n",
    "            GELayerS2(64, 128),\n",
    "            GELayerS1(128, 128),\n",
    "            GELayerS1(128, 128),\n",
    "            GELayerS1(128, 128),\n",
    "        )\n",
    "        self.S5_5 = CEBlock()\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat2 = self.S1S2(x)  # (N, 16, h/4, w/4)\n",
    "        feat3 = self.S3(feat2)  # (N, 32, h/8. w/8)\n",
    "        feat4 = self.S4(feat3)  # (N, 64, h/16, w/16)\n",
    "        feat5_4 = self.S5_4(feat4)  # (N, 128, h/32, w/32 )\n",
    "        feat5_5 = self.S5_5(feat5_4)  # (N, 128, h/32, w/32)\n",
    "\n",
    "        return feat2, feat3, feat4, feat5_4, feat5_5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db11a6a",
   "metadata": {},
   "source": [
    "## 3. Bilateral Guided Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e024f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGALayer(nn.Module):\n",
    "    '''fuse the complementary information from detail & semantic branches\n",
    "       detail-branch-(N, H, W, 128) / semantic-branch-(N, H/4, W/4, 128)'''\n",
    "    def __init__(self):\n",
    "        super(BGALayer, self).__init__()\n",
    "        \n",
    "        # ---- Detail Branch ----\n",
    "        # 1. 3x3 depth-wise conv & 1x1 conv\n",
    "        self.left1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                128, 128, kernel_size=3, stride=1,\n",
    "                padding=1, groups=128, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(\n",
    "                128, 128, kernel_size=1, stride=1,\n",
    "                padding=0, bias=False),\n",
    "        )  # (N, H, W, 128)\n",
    "        \n",
    "        # 2. 3x3 conv with stride=2 & 3x3 APooling\n",
    "        self.left2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                128, 128, kernel_size=3, stride=2,\n",
    "                padding=1, bias=False),  # (N, H/2, W/2, 128)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.AvgPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=False)  # (N, H/4, W/4, 128)\n",
    "        )  \n",
    "        \n",
    "        # ---- Semantic Branch ----\n",
    "        # 1. 3x3 conv & 4x4 up-sample\n",
    "        self.right1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                128, 128, kernel_size=3, stride=1,\n",
    "                padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "        )  # (N, H/4, W/4, 128)\n",
    "        self.up1 = nn.Upsample(scale_factor=4)  # (N, H, W, 128)\n",
    "        \n",
    "        # 2. 3x3 depth-wise conv & 1x1 conv\n",
    "        self.right2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                128, 128, kernel_size=3, stride=1,\n",
    "                padding=1, groups=128, bias=False),  # (N, H/4, W/4, 128)\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(\n",
    "                128, 128, kernel_size=1, stride=1,\n",
    "                padding=0, bias=False),  # (N, H/4, W/4, 128)\n",
    "        )   \n",
    "        \n",
    "        self.up2 = nn.Upsample(scale_factor=4)  # use before summation\n",
    "        \n",
    "        ##TODO: does this really has no relu?\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                128, 128, kernel_size=3, stride=1,\n",
    "                padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True), # not shown in paper\n",
    "        )\n",
    "\n",
    "    def forward(self, x_d, x_s):  \n",
    "        '''x_d: output of detail-branch (N, 128, h/8, w/8)\n",
    "           x_s: output of semantic-branch (N, 128, h/32, w/32)'''\n",
    "        dsize = x_d.size()[2:]\n",
    "        \n",
    "        # Detail Branch\n",
    "        left1 = self.left1(x_d)  # (N, 128, h/8, w/8)\n",
    "        left2 = self.left2(x_d)  # (N, 128, h/32, w/32)\n",
    "\n",
    "        # Semantic Branch\n",
    "        right1 = self.right1(x_s)  # (N, 128, h/32, w/32)\n",
    "        right1 = self.up1(right1)  # (N, 128, h/8, w/8)\n",
    "\n",
    "        right2 = self.right2(x_s)  # (N, 128, h/32, w/32)\n",
    "        \n",
    "        # Fuse\n",
    "        left = left1 * torch.sigmoid(right1)  # (N, 128, h/8, w/8)\n",
    "        \n",
    "        right = left2 * torch.sigmoid(right2)  # (N, 128, h/32, w/32)\n",
    "        right = self.up2(right)  # (N, 128, h/8, w/8)\n",
    "        \n",
    "        out = self.conv(left + right)  # (N, 128, h/8, w/8)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48872fb",
   "metadata": {},
   "source": [
    "## 4. Segment Head in Booster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76482c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegmentHead(nn.Module):\n",
    "\n",
    "    def __init__(self, in_chan, mid_chan, n_classes, up_factor=8, aux=True):\n",
    "        '''aux=False for final prediction, aug=True for Booster'''\n",
    "        super(SegmentHead, self).__init__()\n",
    "        self.conv = ConvBNReLU(in_chan, mid_chan, 3, stride=1)\n",
    "        self.drop = nn.Dropout(0.1)\n",
    "        self.up_factor = up_factor\n",
    "\n",
    "        out_chan = n_classes\n",
    "        mid_chan2 = up_factor * up_factor if aux else mid_chan\n",
    "        up_factor = up_factor // 2 if aux else up_factor\n",
    "        self.conv_out = nn.Sequential(\n",
    "            nn.Sequential(\n",
    "                nn.Upsample(scale_factor=2),\n",
    "                ConvBNReLU(mid_chan, mid_chan2, 3, stride=1)\n",
    "                ) if aux else nn.Identity(),\n",
    "            nn.Conv2d(mid_chan2, out_chan, 1, 1, 0, bias=True),\n",
    "            nn.Upsample(scale_factor=up_factor, mode='bilinear', align_corners=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''x: (N, 128, h/8, w/8) - output of aggregation layer'''\n",
    "        feat = self.conv(x)  # (N, 128x8, h/8, w/8 )\n",
    "        feat = self.drop(feat)\n",
    "        feat = self.conv_out(feat)  # (N, n_classes, h, w)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d47f9d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1235f4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiSeNetV2(nn.Module):\n",
    "\n",
    "    def __init__(self, n_classes, aux_mode='train'):\n",
    "        super(BiSeNetV2, self).__init__()\n",
    "        self.aux_mode = aux_mode\n",
    "        self.detail = DetailBranch()\n",
    "        self.segment = SegmentBranch()\n",
    "        self.bga = BGALayer()\n",
    "\n",
    "        ## TODO: what is the number of mid chan ?\n",
    "        self.head = SegmentHead(128, 1024, n_classes, up_factor=8, aux=False)\n",
    "        if self.aux_mode == 'train':\n",
    "            self.aux2 = SegmentHead(16, 128, n_classes, up_factor=4)\n",
    "            self.aux3 = SegmentHead(32, 128, n_classes, up_factor=8)\n",
    "            self.aux4 = SegmentHead(64, 128, n_classes, up_factor=16)\n",
    "            self.aux5_4 = SegmentHead(128, 128, n_classes, up_factor=32)\n",
    "            \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        size = x.size()[2:]\n",
    "        # Detail Branch\n",
    "        feat_d = self.detail(x)  # (N, 128, h/8, w/8)\n",
    "        \n",
    "        # Semantic Branch\n",
    "        feat2, feat3, feat4, feat5_4, feat_s = self.segment(x)\n",
    "        # feat2: (N, 16, h/4, w/4) - stem block\n",
    "        # feat3: (N, 32, h/8, w/8) - GE\n",
    "        # feat4: (N, 64, h/16, w/16) - GE\n",
    "        # feat5_4: (N, 128, h/32, w/32) - GE\n",
    "        # feat_s: (N, 128, h/32, w/32) - context embedding\n",
    "        \n",
    "        # Bilateral Guided Aggregation\n",
    "        # fuse detail-branch: (N, H, W, C) & semantic-branch: (N, H/4, W/4, C)\n",
    "        feat_head = self.bga(feat_d, feat_s)  # (N, 128, h/8, w/8)\n",
    "\n",
    "        logits = self.head(feat_head)  # (N, n_classes, h, w)\n",
    "        if self.aux_mode == 'train':\n",
    "            logits_aux2 = self.aux2(feat2)  # (N, n_classes, h, w)\n",
    "            logits_aux3 = self.aux3(feat3)\n",
    "            logits_aux4 = self.aux4(feat4)\n",
    "            logits_aux5_4 = self.aux5_4(feat5_4)\n",
    "            return logits, logits_aux2, logits_aux3, logits_aux4, logits_aux5_4\n",
    "        \n",
    "        elif self.aux_mode == 'eval':\n",
    "            return logits,\n",
    "        \n",
    "        elif self.aux_mode == 'pred':\n",
    "            pred = logits.argmax(dim=1)\n",
    "            return pred\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            \n",
    "    def init_weights(self):\n",
    "        for name, module in self.named_modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(module.weight, mode='fan_out')\n",
    "                if not module.bias is None: nn.init.constant_(module.bias, 0)\n",
    "            elif isinstance(module, nn.modules.batchnorm._BatchNorm):\n",
    "                if hasattr(module, 'last_bn') and module.last_bn:\n",
    "                    nn.init.zeros_(module.weight)\n",
    "                else:\n",
    "                    nn.init.ones_(module.weight)\n",
    "                nn.init.zeros_(module.bias)\n",
    "        self.load_pretrain()\n",
    "\n",
    "\n",
    "    def load_pretrain(self):\n",
    "        state = modelzoo.load_url(backbone_url)\n",
    "        for name, child in self.named_children():\n",
    "            if name in state.keys():\n",
    "                child.load_state_dict(state[name], strict=True)\n",
    "\n",
    "    def get_params(self):\n",
    "        def add_param_to_list(mod, wd_params, nowd_params):\n",
    "            for param in mod.parameters():\n",
    "                if param.dim() == 1:\n",
    "                    nowd_params.append(param)\n",
    "                elif param.dim() == 4:\n",
    "                    wd_params.append(param)\n",
    "                else:\n",
    "                    print(name)\n",
    "\n",
    "        wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params = [], [], [], []\n",
    "        for name, child in self.named_children():\n",
    "            if 'head' in name or 'aux' in name:\n",
    "                add_param_to_list(child, lr_mul_wd_params, lr_mul_nowd_params)\n",
    "            else:\n",
    "                add_param_to_list(child, wd_params, nowd_params)\n",
    "        return wd_params, nowd_params, lr_mul_wd_params, lr_mul_nowd_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "138857d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn([2, 3, 512, 512])\n",
    "model = BiSeNetV2(n_classes=1, aux_mode='train')\n",
    "out, aux1, aux2, aux3, aux4 = model(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b88ee18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdab9d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
