{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Dataset&DataLoader.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset \n",
        "\n",
        "\n",
        "PASCAL VOC2012 이용\n",
        "\n",
        "http://host.robots.ox.ac.uk/pascal/VOC/voc2012/"
      ],
      "metadata": {
        "id": "t4NBlQIPntu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile"
      ],
      "metadata": {
        "id": "MLutw7tvoC1w"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data 폴더 작성\n",
        "data_dir = \"./data/\"\n",
        "if not os.path.exists(data_dir):\n",
        "    os.mkdir(data_dir)\n",
        "\n",
        "# weights 폴더 작성\n",
        "weights_dir = \"./weights/\"\n",
        "if not os.path.exists(weights_dir):\n",
        "    os.mkdir(weights_dir)"
      ],
      "metadata": {
        "id": "8Sy5-IX7oC5X"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download\n",
        "url = \"http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\"\n",
        "target_path = os.path.join(data_dir, \"VOCtrainval_11-May-2012.tar\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n",
        "    \n",
        "    tar = tarfile.TarFile(target_path)  \n",
        "    tar.extractall(data_dir)  \n",
        "    tar.close() "
      ],
      "metadata": {
        "id": "ICjTLQ5toC8X"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained된 SSD용 VGG16 파라미터 다운로드\n",
        "url = \"https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth\"\n",
        "target_path = os.path.join(weights_dir, \"vgg16_reducedfc.pth\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)"
      ],
      "metadata": {
        "id": "E-Ufs76_qKrD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained된 SSD300 모델 다운로드\n",
        "url = \"https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth\"\n",
        "target_path = os.path.join(weights_dir, \"ssd300_mAP_77.43_v2.pth\") \n",
        "\n",
        "if not os.path.exists(target_path):\n",
        "    urllib.request.urlretrieve(url, target_path)\n"
      ],
      "metadata": {
        "id": "OI08BJK3qU82"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os.path as osp\n",
        "import random\n",
        "# 파일이나 텍스트에서 XML을 읽고, 가공하고 저장하는 라이브러리\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "0ZfRxvoQqgQN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. 이미지 & annotation 데이터의 파일 경로 리스트 작성\n",
        "\n",
        "목적 : 모든 이미지 데이터, annotation 파일의 경로를 리스트형 변수로 작성\n",
        "\n",
        "-> train_img_list, train_anno_list, val_img_list, val_anno_list 출력\n",
        "\n",
        "- Object detection은 이미지 데이터와 대응되는 annotation 데이터를 데이터셋에서 함께 처리\n",
        "\n",
        "- 파일, 폴더 이름에 정답 클래스명이 포함\n",
        "\n",
        "- 객체 위치(bounding box), 정답 클래스 정보는 annotation 데이터로 제공"
      ],
      "metadata": {
        "id": "E0bpisTTq8zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1234)\n",
        "np.random.seed(1234)\n",
        "random.seed(1234)"
      ],
      "metadata": {
        "id": "DeGGM9tWq0D6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_datapath_list(rootpath):\n",
        "    # 이미지, annotation 파일 경로\n",
        "    imgpath_template = osp.join(rootpath, 'JPEGImages', '%s.jpg')  # ex) 2007_000027.jpg\n",
        "    annopath_template = osp.join(rootpath, 'Annotations', '%s.xml')\n",
        "    \n",
        "    # 파일 ID\n",
        "    train_id_names = osp.join(rootpath + 'ImageSets/Main/train.txt')  # ex) 2007_000027\n",
        "    val_id_names = osp.join(rootpath + 'ImageSets/Main/val.txt')\n",
        "    \n",
        "    # train용\n",
        "    train_img_list = []\n",
        "    train_anno_list = []\n",
        "\n",
        "    for line in open(train_id_names):\n",
        "        file_id = line.strip()\n",
        "        img_path = (imgpath_template % file_id)\n",
        "        anno_path = (annopath_template % file_id) \n",
        "        \n",
        "        train_img_list.append(img_path) \n",
        "        train_anno_list.append(anno_path) \n",
        "\n",
        "    # val용\n",
        "    val_img_list = []\n",
        "    val_anno_list = []\n",
        "\n",
        "    for line in open(val_id_names):\n",
        "        file_id = line.strip() \n",
        "        img_path = (imgpath_template % file_id)  \n",
        "        anno_path = (annopath_template % file_id) \n",
        "\n",
        "        val_img_list.append(img_path)  \n",
        "        val_anno_list.append(anno_path)  \n",
        "\n",
        "    return train_img_list, train_anno_list, val_img_list, val_anno_list"
      ],
      "metadata": {
        "id": "R6TSDrc4q3tK"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rootpath = \"./data/VOCdevkit/VOC2012/\"\n",
        "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(rootpath)\n",
        "\n",
        "# 각 이미지 파일에 대응되는 annotation 파일 존재\n",
        "print(train_img_list[0:2])\n",
        "print(train_anno_list[0:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4mK1Zvls2Of",
        "outputId": "6f667b17-7fe4-40b5-dfae-7f7dbbf480c1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['./data/VOCdevkit/VOC2012/JPEGImages/2008_000008.jpg', './data/VOCdevkit/VOC2012/JPEGImages/2008_000015.jpg']\n",
            "['./data/VOCdevkit/VOC2012/Annotations/2008_000008.xml', './data/VOCdevkit/VOC2012/Annotations/2008_000015.xml']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. XML 형식 annotation 데이터를 리스트 형식으로 \n",
        "\n",
        "목적 : xml 형식의 annotation data의 bbox 좌표, 정답 클래스 정보를 리스트 형식으로 저장\n",
        "\n",
        "-> [[xmin, ymin, xmax, ymax, label_ind], ... ]  : 한 장의 이미지에 대한 결과"
      ],
      "metadata": {
        "id": "VeI82hfwuIRG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://user-images.githubusercontent.com/44194558/147519071-e187c42d-3206-4cae-93f3-db20ae86050d.png)"
      ],
      "metadata": {
        "id": "j1xbPFEqvoeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Anno_xml2list(object):\n",
        "\n",
        "    def __init__(self, classes):\n",
        "        self.classes = classes\n",
        "\n",
        "    def __call__(self, xml_path, width, height):\n",
        "        ret = []\n",
        "        xml = ET.parse(xml_path).getroot()\n",
        "        \n",
        "        # 하나의 이미지에 존재하는 모든 객체의 정보를 처리\n",
        "        for obj in xml.iter('object'):\n",
        "            difficult = int(obj.find('difficult').text)\n",
        "            if difficult == 1:\n",
        "                continue\n",
        "            \n",
        "            bndbox = []\n",
        "\n",
        "            name = obj.find('name').text.lower().strip()\n",
        "            bbox = obj.find('bndbox')\n",
        "            \n",
        "            # annotation 파일의 bbox 좌표를 0~1로 정규화\n",
        "            pts = ['xmin', 'ymin', 'xmax', 'ymax']\n",
        "\n",
        "            for pt in (pts):\n",
        "                cur_pixel = int(bbox.find(pt).text) - 1  # VOC는 원점이 (1, 1)\n",
        "\n",
        "                if pt == 'xmin' or pt == 'xmax':\n",
        "                    cur_pixel /= width  # 이미지의 폭으로 나눠서 정규화\n",
        "\n",
        "                else:\n",
        "                    cur_pixel /= height  # 이미지의 높이로 나눠서 정규화\n",
        "\n",
        "                bndbox.append(cur_pixel)\n",
        "\n",
        "            label_idx = self.classes.index(name)\n",
        "            bndbox.append(label_idx)\n",
        "\n",
        "            ret += [bndbox]\n",
        "\n",
        "        return np.array(ret)  # [[xmin, ymin, xmax, ymax, label_ind], ... ], 객체가 여러개 있을 경우 여러 개의 내부 리스트 존재"
      ],
      "metadata": {
        "id": "vVQ10VjOs8Nc"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class 정의\n",
        "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
        "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
        "               'cow', 'diningtable', 'dog', 'horse',\n",
        "               'motorbike', 'person', 'pottedplant',\n",
        "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "transform_anno = Anno_xml2list(voc_classes)"
      ],
      "metadata": {
        "id": "zdEOZBN-s9Kv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인용 예시\n",
        "ind = 1\n",
        "image_file_path = val_img_list[ind]\n",
        "img = cv2.imread(image_file_path)  # [높이][폭][BGR]\n",
        "height, width, channels = img.shape \n",
        "\n",
        "transform_anno(val_anno_list[ind], width, height)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTRsHmNWxkSS",
        "outputId": "4e94d421-7abe-4f4d-ca96-ae8a98e49106"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.09      ,  0.03003003,  0.998     ,  0.996997  , 18.        ],\n",
              "       [ 0.122     ,  0.56756757,  0.164     ,  0.72672673, 14.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 이미지, annotation 전처리\n",
        "\n",
        "이미지와 bbox 정보 변환 수행. 학습, 추론 시에 다르게 작동하도록.\n",
        "\n",
        " - 학습 시에는 DataTransform으로 데이터 augmentation 수행\n",
        "   - Augmentation에서 이미지의 색과 크기가 변경되기 때문에, 이에 대응되는 annotation의 bbox 좌표 정보도 같이 변경되어야 함\n",
        "\n",
        " - 추론 시에는 이미지의 크기를 변경하고 색상 정보의 평균값을 뺌"
      ],
      "metadata": {
        "id": "nMj73q7Jx-tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.data_augumentation import Compose, ConvertFromInts, ToAbsoluteCoords, PhotometricDistort, Expand, RandomSampleCrop, RandomMirror, ToPercentCoords, Resize, SubtractMeans\n",
        "\n",
        "class DataTransform(): \n",
        "    \n",
        "    def __init__(self, input_size, color_mean):\n",
        "        self.data_transform = {\n",
        "            'train': Compose([\n",
        "                ConvertFromInts(),  # int를 float32로 변환\n",
        "                ToAbsoluteCoords(),  # 정규화된 annotation 데이터\n",
        "                PhotometricDistort(),  # 이미지의 색 등을 임의로 변화시킴\n",
        "                Expand(color_mean),  # 이미지의 캔버스 확대\n",
        "                RandomSampleCrop(),  # 이미지 내의 특정 부분을 무작위로 추출\n",
        "                RandomMirror(),  # 이미지를 반전\n",
        "                ToPercentCoords(),  # annotation 데이터를 0-1로 정규화\n",
        "                Resize(input_size),  # 이미지 크기를 input_size × input_size로 변형\n",
        "                SubtractMeans(color_mean)  # BGR 색상의 평균값을 뺀다\n",
        "            ]),\n",
        "\n",
        "            'val': Compose([\n",
        "                ConvertFromInts(),  # int를 float로 변환\n",
        "                Resize(input_size),  # 이미지 크기를 input_size × input_size로 변형\n",
        "                SubtractMeans(color_mean)  # BGR 색상의 평균값을 뺀다\n",
        "            ])\n",
        "        }\n",
        "\n",
        "    def __call__(self, img, phase, boxes, labels):\n",
        "        return self.data_transform[phase](img, boxes, labels)"
      ],
      "metadata": {
        "id": "5cIxyRpU1fT-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인용 예시\n",
        "image_file_path = train_img_list[0]\n",
        "img = cv2.imread(image_file_path)  # [높이][폭][색BGR]\n",
        "height, width, channels = img.shape \n",
        "\n",
        "# 2. annotation을 리스트로\n",
        "transform_anno = Anno_xml2list(voc_classes)\n",
        "anno_list = transform_anno(train_anno_list[0], width, height)\n",
        "\n",
        "# 3. 원본 이미지\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "# 4. 전처리 클래스 \n",
        "color_mean = (104, 117, 123)  # (BGR) 색상의 평균값\n",
        "input_size = 300  # 이미지 input 사이즈를 300×300으로\n",
        "transform = DataTransform(input_size, color_mean)\n",
        "\n",
        "# 5. train 이미지 표시\n",
        "phase = \"train\"\n",
        "img_transformed, boxes, labels = transform(\n",
        "    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
        "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# 6. val 이미지 표시\n",
        "phase = \"val\"\n",
        "img_transformed, boxes, labels = transform(\n",
        "    img, phase, anno_list[:, :4], anno_list[:, 4])\n",
        "plt.imshow(cv2.cvtColor(img_transformed, cv2.COLOR_BGR2RGB))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FS6wqQhRy8-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Dataset\n",
        "\n",
        "목적 : VOCDataset 클래스 생성\n",
        "\n",
        " - 앞에서 정의한 클래스 활용"
      ],
      "metadata": {
        "id": "74YXs7qG04J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VOCDataset(data.Dataset):\n",
        "    \"\"\"\n",
        "    Attributes\n",
        "    ----------\n",
        "    img_list : 리스트\n",
        "        이미지 경로를 저장한 리스트\n",
        "    anno_list : 리스트\n",
        "        annotation 경로를 저장한 리스트\n",
        "    phase : 'train' or 'test'\n",
        "        train/test 설정\n",
        "    transform : object\n",
        "        전처리 클래스의 인스턴스\n",
        "    transform_anno : object\n",
        "        xml annotation을 리스트로 변환\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_list, anno_list, phase, transform, transform_anno):\n",
        "        self.img_list = img_list\n",
        "        self.anno_list = anno_list\n",
        "        self.phase = phase  \n",
        "        self.transform = transform  \n",
        "        self.transform_anno = transform_anno  \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_list)\n",
        "    \n",
        "    # 전처리 완료된 이미지 tensor, annotation\n",
        "    def __getitem__(self, index):\n",
        "        im, gt, h, w = self.pull_item(index)\n",
        "        return im, gt\n",
        "    \n",
        "    # 전처리 완료된 이미지 tensor, annotation, h, w\n",
        "    def pull_item(self, index):\n",
        "        # 하나의 이미지 읽기\n",
        "        image_file_path = self.img_list[index]\n",
        "        img = cv2.imread(image_file_path)  # [높이][폭][색BGR]\n",
        "        height, width, channels = img.shape  \n",
        "        \n",
        "        # 개별 이미지에 대응되는 annotation\n",
        "        anno_file_path = self.anno_list[index]\n",
        "        anno_list = self.transform_anno(anno_file_path, width, height)\n",
        "        \n",
        "        # 전처리\n",
        "        img, boxes, labels = self.transform(\n",
        "            img, self.phase, anno_list[:, :4], anno_list[:, 4])\n",
        "        \n",
        "        # BGR -> RGB\n",
        "        # (h, w, c) -> (c, h, w)\n",
        "        img = torch.from_numpy(img[:, :, (2, 1, 0)]).permute(2, 0, 1)\n",
        "\n",
        "        gt = np.hstack((boxes, np.expand_dims(labels, axis=1)))\n",
        "\n",
        "        return img, gt, height, width"
      ],
      "metadata": {
        "id": "tQtKVI1wy9Cw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 확인용 예시\n",
        "color_mean = (104, 117, 123)  # (BGR) 색의 평균값\n",
        "input_size = 300  \n",
        "\n",
        "train_dataset = VOCDataset(train_img_list, train_anno_list, phase=\"train\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DataTransform(\n",
        "    input_size, color_mean), transform_anno=Anno_xml2list(voc_classes))\n",
        "\n",
        "val_dataset.__getitem__(1)"
      ],
      "metadata": {
        "id": "lhUN6T2Py9E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataLoader\n",
        "\n",
        "Dataset에서 꺼내는 annotation 데이터의 크기는 이미지 마다 다름. (이미지 내에 존재하는 객체의 수가 다르기 때문에)"
      ],
      "metadata": {
        "id": "sshhBTeg4oiS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def od_collate_fn(batch):\n",
        "\n",
        "    targets = []\n",
        "    imgs = []\n",
        "    for sample in batch:\n",
        "        imgs.append(sample[0])  # sample[0]는 이미지\n",
        "        targets.append(torch.FloatTensor(sample[1]))  # sample[1]는 annotation gt\n",
        "\n",
        "    # imgs는 미니 배치 크기의 리스트\n",
        "    # 리스트의 개별 요소는 torch.Size([3, 300, 300]) - 300x300 크기의 칼라 이미지\n",
        "    # imgs 리스트를 torch.Size([batch_num, 3, 300, 300])의 텐서로 변환\n",
        "    imgs = torch.stack(imgs, dim=0)\n",
        "\n",
        "    # targets는 annotation gt의 리스트\n",
        "    # 리스트의 크기는 미니 배치의 크기\n",
        "    # targets의 개별 요소는 [n, 5] 크기 (bbox 좌표 4개 + 정답 클래스 1개)\n",
        "    # n은 이미지마다 다름 (이미지 내에 존재하는 객체의 수)\n",
        "\n",
        "    return imgs, targets\n"
      ],
      "metadata": {
        "id": "5wZjBhLP4rHg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4\n",
        "\n",
        "train_dataloader = data.DataLoader(\n",
        "    train_dataset, batch_size=batch_size, shuffle=True, collate_fn=od_collate_fn)\n",
        "\n",
        "val_dataloader = data.DataLoader(\n",
        "    val_dataset, batch_size=batch_size, shuffle=False, collate_fn=od_collate_fn)\n",
        "\n",
        "# 사전형 변수에 정리\n",
        "dataloaders_dict = {\"train\": train_dataloader, \"val\": val_dataloader}\n",
        "\n",
        "# 동작 확인\n",
        "batch_iterator = iter(dataloaders_dict[\"val\"])  # 반복자로 변환\n",
        "images, targets = next(batch_iterator)  # 첫 번째 요소를 추출\n",
        "print(images.size())  # torch.Size([4, 3, 300, 300])\n",
        "print(len(targets))\n",
        "print(targets[1].size())  "
      ],
      "metadata": {
        "id": "CkiVgbDY7FIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.__len__())\n",
        "print(val_dataset.__len__())"
      ],
      "metadata": {
        "id": "j0dyYUXH7MAc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}