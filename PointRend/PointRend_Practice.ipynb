{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PointRend.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YUaevniauCsK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Backbone Network"
      ],
      "metadata": {
        "id": "5f87j2AbuRxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ResNet\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models.resnet import ResNet, Bottleneck\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "# DeepLab\n",
        "from torchvision.models._utils import IntermediateLayerGetter\n",
        "from torch.hub import load_state_dict_from_url\n",
        "from torchvision.models.segmentation._utils import _SimpleSegmentationModel\n",
        "from torchvision.models.segmentation.deeplabv3 import DeepLabHead\n",
        "from torchvision.models.segmentation.fcn import FCNHead\n",
        "from torchvision.models import resnet50, resnet101"
      ],
      "metadata": {
        "id": "9TMTLppquGdr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNetXX3(ResNet):\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
        "                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n",
        "                 norm_layer=None):\n",
        "        super().__init__(block, layers, num_classes, zero_init_residual,\n",
        "                         groups, width_per_group, replace_stride_with_dilation,\n",
        "                         norm_layer)\n",
        "        self.conv1 = nn.Conv2d(3, 64, 3, 1, 1, bias=False)\n",
        "        nn.init.kaiming_normal_(self.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
        "\n",
        "\n",
        "def resnet53(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-50 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return ResNetXX3(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
        "\n",
        "\n",
        "def resnet103(pretrained=False, progress=True, **kwargs):\n",
        "    r\"\"\"ResNet-101 model from\n",
        "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    return ResNetXX3(Bottleneck, [3, 4, 23, 3], **kwargs)"
      ],
      "metadata": {
        "id": "Iq-c91-auOpS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmallDeepLab(_SimpleSegmentationModel):\n",
        "    def forward(self, input_):\n",
        "        result = self.backbone(input_)\n",
        "        result[\"coarse\"] = self.classifier(result[\"out\"])\n",
        "        return result\n",
        "\n",
        "def deeplabv3(pretrained=False, resnet=\"res103\", head_in_ch=2048, num_classes=21):\n",
        "    resnet = {\n",
        "        \"res53\":  resnet53,\n",
        "        \"res103\": resnet103,\n",
        "        \"res50\":  resnet50,\n",
        "        \"res101\": resnet101\n",
        "    }[resnet]\n",
        "\n",
        "    net = SmallDeepLab(\n",
        "        # Backbone\n",
        "        backbone = IntermediateLayerGetter(\n",
        "            resnet(pretrained=True, replace_stride_with_dilation=[False, True, True]),\n",
        "            return_layers={'layer2': 'res2', 'layer4': 'out'}  \n",
        "        ),\n",
        "\n",
        "        # Classifier (layer4의 출력을 입력으로 받음)\n",
        "        classifier = DeepLabHead(head_in_ch, num_classes)  # coarse mask/prediction 출력\n",
        "    )\n",
        "\n",
        "    return net"
      ],
      "metadata": {
        "id": "u_JBp0uDuXF6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = deeplabv3(False).cuda()\n",
        "x = torch.randn(3, 3, 256, 256).cuda()\n",
        "result = net(x)\n",
        "#net"
      ],
      "metadata": {
        "id": "y-S3gbV-ucNV"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res2 = result['res2']  \n",
        "out = result['coarse'] \n",
        "\n",
        "print('layer2 :', res2.shape)  # intermediate features\n",
        "print('coarse :', out.shape)  # coarse features (mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQ9POM5Cui1V",
        "outputId": "93c9ab84-fd34-4cf7-b473-61a5e4200edd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "layer2 : torch.Size([3, 512, 64, 64])\n",
            "coarse : torch.Size([3, 21, 64, 64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Point Selection"
      ],
      "metadata": {
        "id": "jCB-MYHmzugF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Train : feature map에서 학습할 N개의 point 샘플링\n",
        "\n",
        "Inference : N개의 가장 **불확실한** (이웃 값과 값이 달라질 가능성이 높은, binary mask의 확률이 0.5에 가까운) point 샘플링\n",
        "\n",
        "<br/>\n",
        "\n",
        "**Sampling Strategy (Training)**\n",
        "\n",
        "![image](https://user-images.githubusercontent.com/44194558/158567732-a03c3005-cf54-4499-bdc5-b1244ed60de1.png)\n",
        "\n",
        "참고 : https://doooob.tistory.com/79"
      ],
      "metadata": {
        "id": "2iuiFJNI0QeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "KfhIxCKD2Z0x"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def point_sample(input, point_coords, **kwargs):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        input (Tensor): (N, C, H, W) - (3, 21, 64, 64) coarse mask\n",
        "        point_coords (Tensor): (N, P, 2) - 선택된 모든 point들의 normalize된 좌표 정보\n",
        "\n",
        "    Output:\n",
        "        output (Tensor): (N, C, P) - 선택된 모든 P개의 point들의 point-wise feature (mask에 대해 bi-linear interpolation을 통해 계산됨)\n",
        "                         `torch.nn.functional.grid_sample` 함수를 이용하여 계산 (https://pytorch.org/docs/stable/generated/torch.nn.functional.grid_sample.html)\n",
        "    \"\"\"\n",
        "    add_dim = False\n",
        "    if point_coords.dim() == 3:\n",
        "        add_dim = True\n",
        "        point_coords = point_coords.unsqueeze(2)  # (3, 96, 2) -> (3, 96, 1, 2)\n",
        "    \n",
        "    # coarse mask에서 point_coords에 저장된 point 위치 정보를 이용하여 point 별로 feature vector 계산\n",
        "    output = F.grid_sample(input, 2.0 * point_coords - 1.0, **kwargs)  # (3, 21, 96, 1)\n",
        "    if add_dim:\n",
        "        output = output.squeeze(3)  # (3, 21, 96) - 96개 point들의 21차원 feature vector\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "TrsBz7NT1gNK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def sampling_points(mask, N, k=3, beta=0.75, training=True):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        mask(Tensor): [B, C, H, W] - Coarse mask\n",
        "        N(int): `During training we sample as many points as there are on a stride 16 feature map of the input`\n",
        "\n",
        "        'Sampling strategy 참고'\n",
        "         - k(int): Over generation multiplier (k > 1) \n",
        "         - beta(float): ratio of importance points\n",
        "\n",
        "        training(bool): flag\n",
        "\n",
        "    Return:\n",
        "        selected_point(Tensor) : flattened indexing points [B, num_points, 2]\n",
        "    \"\"\"\n",
        "    \n",
        "    assert mask.dim() == 4,\"Dim must be N(Batch)CHW\"\n",
        "    device = mask.device\n",
        "    B, _, H, W = mask.shape  # coarse mask/prediction - (3, 21, 64, 64)\n",
        "    mask, _ = mask.sort(1, descending=True)  # 채널 차원으로 내림차순 정렬 (importance sampling의 uncertatinty 계산과 관련 있음)\n",
        "    \n",
        "    # Inference\n",
        "    if not training:\n",
        "        H_step, W_step = 1 / H, 1 / W\n",
        "        N = min(H * W, N)\n",
        "        # uncertatinty 계산 \n",
        "        uncertainty_map = -1 * (mask[:, 0] - mask[:, 1])\n",
        "        _, idx = uncertainty_map.view(B, -1).topk(N, dim=1)  # N개의 불확실한 point 선택\n",
        "\n",
        "        points = torch.zeros(B, N, 2, dtype=torch.float, device=device)\n",
        "        points[:, :, 0] = W_step / 2.0 + (idx  % W).to(torch.float) * W_step\n",
        "        points[:, :, 1] = H_step / 2.0 + (idx // W).to(torch.float) * H_step\n",
        "\n",
        "        return idx, points\n",
        "    \n",
        "    # 1. Over-generation : kN개의 point를 무작위로 샘플링 (N=32, k=3) \n",
        "    over_generation = torch.randn(B, k * N, 2, device=device)  # (3, 96, 2) - 96개 point의 좌표 정보\n",
        "    over_generation_map = point_sample(mask, over_generation, align_corners=False)  # (3, 21, 96) - point wise features\n",
        "    \n",
        "    # 2. Importance sampling - 96개의 over-sampled point중 불확실성의 정도가 높은 24개(Nxbeta)의 point 선별\n",
        "    # topk : 주어진 차원을 따라 텐서 중 특정 개수의 가장 큰 요소 반환 (https://runebook.dev/ko/docs/pytorch/generated/torch.topk)\n",
        "    uncertainty_map = -1 * (over_generation_map[:, 0] - over_generation_map[:, 1])  # the diff btw the most confident & second most confident class probabilities - (3, 96)\n",
        "    _, idx = uncertainty_map.topk(int(beta * N), -1)  # (3, 24) - 96개 point중 24개 선택\n",
        "\n",
        "    shift = (k * N) * torch.arange(B, dtype=torch.long, device=device)\n",
        "    idx += shift[:, None]\n",
        "    importance = over_generation.view(-1, 2)[idx.view(-1), :].view(B, int(beta * N), 2)  # (3, 24, 2)\n",
        "\n",
        "    # 3. Coverage : uniform dist로부터 나머지 8개(Nx(1-beta)) point 선별 \n",
        "    coverage = torch.rand(B, N - int(beta * N), 2, device=device)  # (3, 8, 2)\n",
        "\n",
        "    return torch.cat([importance, coverage], 1).to(device)  # 최종적으로 샘플링된 32개의 point"
      ],
      "metadata": {
        "id": "-P8NYkuBwWe9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. PointRend"
      ],
      "metadata": {
        "id": "nO9j8NBsP22w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 PointHead\n",
        "\n",
        "`Given the point-wise feature representation at each selected point, PointRend makes point-wise segmentation predictions using a simple MLP.`\n",
        "\n",
        "<br/>\n",
        "\n",
        "2.Point Selection의 sampling_points, point_sample 활용"
      ],
      "metadata": {
        "id": "lS9LYBs2RQJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointHead(nn.Module):\n",
        "    def __init__(self, in_c=533, num_classes=21, k=3, beta=0.75):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Conv1d(in_c, num_classes, 1)\n",
        "        self.k = k\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, x, res2, out):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            x : (B, C, H, W)\n",
        "            res2 : backbone network의 layer2 출력\n",
        "            out : coarse mask (prediction)\n",
        "        \"\"\"\n",
        "        if not self.training:\n",
        "            return self.inference(x, res2, out)\n",
        "        \n",
        "        # sampling_points(mask, N, k=3, beta=0.75, training=True):\n",
        "        points = sampling_points(out, x.shape[-1] // 16, self.k, self.beta)  # 샘플링된 32개 point\n",
        "        \n",
        "        # point-wise features\n",
        "        coarse = point_sample(out, points, align_corners=False)  # (3, 21, 32)  \n",
        "        fine = point_sample(res2, points, align_corners=False)  # (3, 512, 32)\n",
        "        \n",
        "        # coarse prediction feature, fine-grained feature 결합\n",
        "        feature_representation = torch.cat([coarse, fine], dim=1)  # (3, 533, 32)\n",
        "\n",
        "        rend = self.mlp(feature_representation)\n",
        "\n",
        "        return {\"rend\": rend,  # (3, 21, 16) \n",
        "                \"points\": points}  # (3, 16, 2)"
      ],
      "metadata": {
        "id": "q0DtVQoINMhd"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Network"
      ],
      "metadata": {
        "id": "Gu0ChJBYUkak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PointRend(nn.Module):\n",
        "    def __init__(self, backbone, head):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.head = head  # PointHead\n",
        "\n",
        "    def forward(self, x):\n",
        "        result = self.backbone(x)  # backbone network 통과\n",
        "        result.update(self.head(x, result[\"res2\"], result[\"coarse\"]))\n",
        "        \n",
        "        return result"
      ],
      "metadata": {
        "id": "F6xTXrJOPkXH"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, 3, 256, 256).cuda()\n",
        "net = PointRend(deeplabv3(False), PointHead()).cuda()\n",
        "out = net(x)"
      ],
      "metadata": {
        "id": "XdcPaHwcPkaO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for k, v in out.items():\n",
        "    print(k, ':', v.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5It59BdPkcr",
        "outputId": "b51c71a8-7bfd-4e3b-dd35-1ffd59074ab8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "res2 : torch.Size([3, 512, 64, 64])\n",
            "out : torch.Size([3, 2048, 64, 64])\n",
            "coarse : torch.Size([3, 21, 64, 64])\n",
            "rend : torch.Size([3, 21, 16])\n",
            "points : torch.Size([3, 16, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4CD_np3aVbZu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}