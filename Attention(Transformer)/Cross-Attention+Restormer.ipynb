{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14fc640f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GDFN_1(nn.Module):\n",
    "    def __init__(self, channels, expansion_factor):  \n",
    "        super(GDFN_1, self).__init__()\n",
    "\n",
    "        hidden_channels = int(channels * expansion_factor)  # channel expansion \n",
    "        # 1x1 conv to extend feature channel\n",
    "        self.project_in = nn.Conv2d(channels, hidden_channels * 2, kernel_size=1, bias=False)  \n",
    "        \n",
    "        # 3x3 DW Conv (groups=input_channels) -> each input channel is convolved with its own set of filters\n",
    "        self.conv = nn.Conv2d(hidden_channels * 2, hidden_channels * 2, kernel_size=3, padding=1,\n",
    "                              groups=hidden_channels * 2, bias=False)\n",
    "        \n",
    "        # 1x1 conv to reduce channels back to original input dimension\n",
    "        self.project_out = nn.Conv2d(hidden_channels, channels, kernel_size=1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''HxWxC -> HxWxC'''\n",
    "        x1, x2 = self.conv(self.project_in(x)).chunk(2, dim=1)\n",
    "        # Gating: the element-wise product of 2 parallel paths of linear transformation layers \n",
    "        x = self.project_out(F.gelu(x1) * x2)\n",
    "        \n",
    "        return x    \n",
    "    \n",
    "    \n",
    "class MDTA_1(nn.Module):\n",
    "    def __init__(self, channels, num_heads):\n",
    "        super(MDTA_1, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(1, num_heads, 1, 1))\n",
    "\n",
    "        self.qkv = nn.Conv2d(channels, channels * 3, kernel_size=1, bias=False)\n",
    "        self.qkv_conv = nn.Conv2d(channels * 3, channels * 3, kernel_size=3, padding=1, groups=channels * 3, bias=False)  # DConv\n",
    "\n",
    "        self.project_out_1 = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
    "        self.project_out_2 = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
    "        self.project_out_3 = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
    "        self.project_out_4 = nn.Conv2d(channels, channels, kernel_size=1, bias=False)\n",
    "\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        '''x, y: (N, C, H, W) - features to fuse'''\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        q_x, k_x, v_x = self.qkv_conv(self.qkv(x)).chunk(3, dim=1)  # (N, C, H, W)  \n",
    "        q_y, k_y, v_y = self.qkv_conv(self.qkv(y)).chunk(3, dim=1)  # (N, C, H, W)\n",
    "        \n",
    "        # divide the # of channels into heads & learn separate attention map\n",
    "        q_x = q_x.reshape(b, self.num_heads, -1, h * w)  # (N, num_heads, C/num_heads, HW)\n",
    "        k_x = k_x.reshape(b, self.num_heads, -1, h * w)\n",
    "        v_x = v_x.reshape(b, self.num_heads, -1, h * w)\n",
    "\n",
    "        q_y = q_y.reshape(b, self.num_heads, -1, h * w)  # (N, num_heads, C/num_heads, HW)\n",
    "        k_y = k_y.reshape(b, self.num_heads, -1, h * w)\n",
    "        v_y = v_y.reshape(b, self.num_heads, -1, h * w)\n",
    "\n",
    "        q_x, k_x = F.normalize(q_x, dim=-1), F.normalize(k_x, dim=-1)\n",
    "        q_y, k_y = F.normalize(q_y, dim=-1), F.normalize(k_y, dim=-1)\n",
    "        \n",
    "        # SA(Intra) - CxC Self Attention map instead of HWxHW (when num_heads=1)\n",
    "        self_attn_x = torch.softmax(torch.matmul(q_x, k_x.transpose(-2, -1).contiguous()) * self.temperature, dim=-1)  # (N, num_heads, C/num_heads, C_num_heads)\n",
    "        self_attn_y = torch.softmax(torch.matmul(q_y, k_y.transpose(-2, -1).contiguous()) * self.temperature, dim=-1)\n",
    "        \n",
    "        intra_x = self.project_out_1(torch.matmul(self_attn_x, v_x).reshape(b, -1, h, w))  # (N, C, H, W)\n",
    "        intra_y = self.project_out_2(torch.matmul(self_attn_y, v_y).reshape(b, -1, h, w))\n",
    "\n",
    "        # CA(Inter) - CxC Cross Attention map instead of HWxHW (when num_heads=1)\n",
    "        cross_attn_xy = torch.softmax(torch.matmul(q_x, k_y.transpose(-2, -1).contiguous()) * self.temperature, dim=-1)  # (N, num_heads, C/num_heads, C_num_heads)\n",
    "        cross_attn_yx = torch.softmax(torch.matmul(q_y, k_x.transpose(-2, -1).contiguous()) * self.temperature, dim=-1)\n",
    "        cross_attn_yx = cross_attn_yx.squeeze(0)\n",
    "        \n",
    "        inter_xy = self.project_out_3(torch.matmul(cross_attn_xy, v_y).reshape(b, -1, h, w))  # (N, C, H, W)\n",
    "        inter_yx = self.project_out_4(torch.matmul(cross_attn_yx, v_x).reshape(b, -1, h, w))\n",
    "        # out = self.project_out(torch.matmul(attn, v).reshape(b, -1, h, w))\n",
    "\n",
    "        return intra_x, intra_y, inter_xy, inter_yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4513a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads, expansion_factor):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1_1 = nn.LayerNorm(channels)\n",
    "        self.norm1_2 = nn.LayerNorm(channels)\n",
    "        \n",
    "        self.attn = MDTA_1(channels, num_heads)\n",
    "        \n",
    "        self.norm2_1 = nn.LayerNorm(channels)\n",
    "        self.norm2_2 = nn.LayerNorm(channels)\n",
    "        self.norm2_3 = nn.LayerNorm(channels)\n",
    "        self.norm2_4 = nn.LayerNorm(channels)\n",
    "\n",
    "        # parallel GDFNs\n",
    "        self.ffn_1 = GDFN_1(channels, expansion_factor)\n",
    "        self.ffn_2 = GDFN_1(channels, expansion_factor)\n",
    "        self.ffn_3 = GDFN_1(channels, expansion_factor)\n",
    "        self.ffn_4 = GDFN_1(channels, expansion_factor)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        # SA feature-x, SA feature-y, CA feature-(query=x, key=y), CA featue(query=y, key=x)\n",
    "        sa_x, sa_y, cross_xy, cross_yx = self.attn(self.norm1_1(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                                                   .contiguous().reshape(b, c, h, w), self.norm1_2(y.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                                                   .contiguous().reshape(b, c, h, w))\n",
    "        x = x + sa_x\n",
    "        y = y + sa_y\n",
    "        \n",
    "        # GDFNs\n",
    "        x = x + self.ffn_1(self.norm2_1(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                         .contiguous().reshape(b, c, h, w))\n",
    "        \n",
    "        y = y + self.ffn_2(self.norm2_2(y.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                         .contiguous().reshape(b, c, h, w))\n",
    "        \n",
    "        ca_xy = cross_xy + self.ffn_3(self.norm2_3(cross_xy.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                                      .contiguous().reshape(b, c, h, w))\n",
    "        \n",
    "        ca_yx = cross_yx + self.ffn_4(self.norm2_4(cross_yx.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                                      .contiguous().reshape(b, c, h, w))\n",
    "\n",
    "        return x, y, ca_xy, ca_yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ebeed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, channels, num_heads, expansion_factor):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        self.norm1_1 = nn.LayerNorm(channels)\n",
    "        self.norm1_2 = nn.LayerNorm(channels)\n",
    "        \n",
    "        self.attn = MDTA_1(channels, num_heads)\n",
    "        \n",
    "        self.norm2_1 = nn.LayerNorm(channels)\n",
    "        self.norm2_2 = nn.LayerNorm(channels)\n",
    "\n",
    "        # parallel GDFNs\n",
    "        self.ffn_1 = GDFN_1(channels, expansion_factor)\n",
    "        self.ffn_2 = GDFN_1(channels, expansion_factor)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        # SA feature-x, SA feature-y, CA feature-(query=x, key=y), CA featue(query=y, key=x)\n",
    "        _, _, cross_xy, cross_yx = self.attn(self.norm1_1(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                                             .contiguous().reshape(b, c, h, w), self.norm1_2(y.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                                             .contiguous().reshape(b, c, h, w))\n",
    "        x = x + cross_xy\n",
    "        y = y + cross_yx\n",
    "        \n",
    "        # GDFNs\n",
    "        ca_xy = x + self.ffn_1(self.norm2_1(x.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                               .contiguous().reshape(b, c, h, w))\n",
    "        \n",
    "        ca_yx = y + self.ffn_2(self.norm2_2(y.reshape(b, c, -1).transpose(-2, -1).contiguous()).transpose(-2, -1)\n",
    "                               .contiguous().reshape(b, c, h, w))\n",
    "\n",
    "        return ca_xy, ca_yx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ae0672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13320a09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
