{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a296f39",
   "metadata": {},
   "source": [
    "## Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "976a65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from lib.modules.layers import *\n",
    "\n",
    "\n",
    "class RFB_kernel(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, receptive_size=3):\n",
    "        super(RFB_kernel, self).__init__()\n",
    "        self.conv0 = conv(in_channel, out_channel, 1)\n",
    "        # k x k = (1 x k) x (k x 1)\n",
    "        self.conv1 = conv(out_channel, out_channel, kernel_size=(1, receptive_size))\n",
    "        self.conv2 = conv(out_channel, out_channel, kernel_size=(receptive_size, 1))     \n",
    "        # 3x3 conv with dilation=k\n",
    "        self.conv3 = conv(out_channel, out_channel, 3, dilation=receptive_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''(N, in_c, h, w) -> (N, out_c, h, w)'''\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class RFB(nn.Module):\n",
    "    '''Receptive Field Block\n",
    "    Input feature map is forwarde to each receptive field path'''\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(RFB, self).__init__()\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "        # receptive field paths\n",
    "        self.branch0 = conv(in_channel, out_channel, 1)  # 1x1 conv\n",
    "        self.branch1 = RFB_kernel(in_channel, out_channel, 3)  # k=3\n",
    "        self.branch2 = RFB_kernel(in_channel, out_channel, 5)  # k=5\n",
    "        self.branch3 = RFB_kernel(in_channel, out_channel, 7)  # k=7\n",
    "\n",
    "        self.conv_cat = conv(4 * out_channel, out_channel, 3)\n",
    "        self.conv_res = conv(in_channel, out_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''(N, in_c, h, w) -> (N, out_c, h, w)'''\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "\n",
    "        x_cat = self.conv_cat(torch.cat((x0, x1, x2, x3), 1))\n",
    "        x = self.relu(x_cat + self.conv_res(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12cb949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attn(nn.Module):\n",
    "    '''Axial-Attention \n",
    "       - performing non-local operation with respect to the single axis(H/W)'''\n",
    "    def __init__(self, in_channels, mode='hw'):\n",
    "        super(self_attn, self).__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "\n",
    "        self.query_conv = conv(in_channels, in_channels // 8, kernel_size=(1, 1))\n",
    "        self.key_conv = conv(in_channels, in_channels // 8, kernel_size=(1, 1))\n",
    "        self.value_conv = conv(in_channels, in_channels, kernel_size=(1, 1))\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):  # x: (N, C, H, W), c=C/8\n",
    "        batch_size, channel, height, width = x.size()\n",
    "        \n",
    "        axis = 1\n",
    "        if 'h' in self.mode:\n",
    "            axis *= height\n",
    "        if 'w' in self.mode:\n",
    "            axis *= width\n",
    "\n",
    "        view = (batch_size, -1, axis)  \n",
    "        \n",
    "        # 1. axis=h -> Q: (N, H, Wc), K: (N, Wc, H), V: (N, WC, H)\n",
    "        # 2. axis=w -> Q: (N, W, Hc), K: (N, Hc, W), V: (N, HC, W)\n",
    "        projected_query = self.query_conv(x).view(*view).permute(0, 2, 1)  \n",
    "        projected_key = self.key_conv(x).view(*view)  \n",
    "        projected_value = self.value_conv(x).view(*view)  \n",
    "        \n",
    "        # 1. axis=h -> attention: (N, H, H)\n",
    "        # 2. axis=w -> attention: (N, W, W)\n",
    "        attention_map = torch.bmm(projected_query, projected_key)  \n",
    "        attention = self.softmax(attention_map)\n",
    "        \n",
    "        # 1. axis=h -> (N, WC, H) -> (N, C, H, W)\n",
    "        # 2. axis=w -> (N, HC, W) -> (N, C, H, W)\n",
    "        out = torch.bmm(projected_value, attention.permute(0, 2, 1))  \n",
    "        out = out.view(batch_size, channel, height, width)  \n",
    "\n",
    "        out = self.gamma * out + x\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533f760",
   "metadata": {},
   "source": [
    "## PAA-Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4478414",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAA_kernel(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, receptive_size=3):\n",
    "        super(PAA_kernel, self).__init__()\n",
    "        self.conv0 = conv(in_channel, out_channel, 1)\n",
    "        self.conv1 = conv(out_channel, out_channel, kernel_size=(1, receptive_size))\n",
    "        self.conv2 = conv(out_channel, out_channel, kernel_size=(receptive_size, 1))\n",
    "        self.conv3 = conv(out_channel, out_channel, 3, dilation=receptive_size)\n",
    "        \n",
    "        # Parallel Axial Attention (sequentially connected axial-attention)\n",
    "        self.Hattn = self_attn(out_channel, mode='h')\n",
    "        self.Wattn = self_attn(out_channel, mode='w')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        Hx = self.Hattn(x)\n",
    "        Wx = self.Wattn(x)\n",
    "        \n",
    "        # aggregate(parallel connection) with summation\n",
    "        x = self.conv3(Hx + Wx)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class PAA_e(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        super(PAA_e, self).__init__()\n",
    "        self.relu = nn.ReLU(True)\n",
    "        \n",
    "        # RFB strategy & global refinement(attention)\n",
    "        self.branch0 = conv(in_channel, out_channel, 1)\n",
    "        self.branch1 = PAA_kernel(in_channel, out_channel, 3)\n",
    "        self.branch2 = PAA_kernel(in_channel, out_channel, 5)\n",
    "        self.branch3 = PAA_kernel(in_channel, out_channel, 7)\n",
    "        \n",
    "        # aggregation\n",
    "        self.conv_cat = conv(4 * out_channel, out_channel, 3)\n",
    "        self.conv_res = conv(in_channel, out_channel, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''(N, in_c, H, W) -> (N, out_c, H, W)'''\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "\n",
    "        x_cat = self.conv_cat(torch.cat((x0, x1, x2, x3), 1))\n",
    "        x = self.relu(x_cat + self.conv_res(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623f682c",
   "metadata": {},
   "source": [
    "## UACA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399df950",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UACA(nn.Module):\n",
    "    '''Uncertainty Augmented Context Attention\n",
    "      -self-attention which incorporates uncertan area for rich semantic feature extraction wo extra boundary guidance'''\n",
    "    def __init__(self, in_channel, channel):\n",
    "        super(UACA, self).__init__()\n",
    "        self.channel = channel\n",
    "\n",
    "        self.conv_query = nn.Sequential(conv(in_channel, channel, 3, relu=True),\n",
    "                                        conv(channel, channel, 3, relu=True))\n",
    "        self.conv_key = nn.Sequential(conv(in_channel, channel, 1, relu=True),\n",
    "                                      conv(channel, channel, 1, relu=True))\n",
    "        self.conv_value = nn.Sequential(conv(in_channel, channel, 1, relu=True),\n",
    "                                        conv(channel, channel, 1, relu=True))\n",
    "\n",
    "        self.conv_out1 = conv(channel, channel, 3, relu=True)\n",
    "        self.conv_out2 = conv(in_channel + channel, channel, 3, relu=True)\n",
    "        self.conv_out3 = conv(channel, channel, 3, relu=True)\n",
    "        self.conv_out4 = conv(channel, 1, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x, map_):\n",
    "        '''x: input feature (N, 512, H, W)\n",
    "           map_: predicted semantic mask (N, 1, h, w)'''\n",
    "        b, c, h, w = x.shape\n",
    "        \n",
    "        # compute class probability\n",
    "        map_ = F.interpolate(map_, size=x.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        fg = torch.sigmoid(map_)\n",
    "        \n",
    "        p = fg - .5\n",
    "\n",
    "        fg = torch.clip(p, 0, 1)  # foreground\n",
    "        bg = torch.clip(-p, 0, 1)  # background\n",
    "        cg = .5 - torch.abs(p)  # confusion area\n",
    "\n",
    "        prob = torch.cat([fg, bg, cg], dim=1)\n",
    "\n",
    "        # reshape feature & prob\n",
    "        f = x.view(b, h * w, -1)  # (N, hw, 512)\n",
    "        prob = prob.view(b, 3, h * w)  # (N, 3, hw)\n",
    "        \n",
    "        # compute context vector\n",
    "        context = torch.bmm(prob, f).permute(0, 2, 1).unsqueeze(3) # b, 3, c (N, 512, 3, 1)\n",
    "\n",
    "        # k q v compute\n",
    "        query = self.conv_query(x).view(b, self.channel, -1).permute(0, 2, 1)  # (N, 256, 256)\n",
    "        key = self.conv_key(context).view(b, self.channel, -1)  # (N, 256, 3)\n",
    "        value = self.conv_value(context).view(b, self.channel, -1).permute(0, 2, 1)  # (N, 3, 256)\n",
    "\n",
    "        # compute similarity map\n",
    "        sim = torch.bmm(query, key) # b, hw, c x b, c, 2  (N, 256, 3)\n",
    "        sim = (self.channel ** -.5) * sim\n",
    "        sim = F.softmax(sim, dim=-1)  # (N, 256, 3)\n",
    "\n",
    "        # compute refined feature\n",
    "        context = torch.bmm(sim, value).permute(0, 2, 1).contiguous().view(b, -1, h, w)  # (N, 256, H, W)\n",
    "        context = self.conv_out1(context)  # (N, 256, H, W)\n",
    "\n",
    "        x = torch.cat([x, context], dim=1)  # (N, 512+256, H, W)\n",
    "        x = self.conv_out2(x)  # (N, 256, H, W)\n",
    "        x = self.conv_out3(x)  # (N, 256, H, W)\n",
    "        out = self.conv_out4(x)  # (N, 1, H, W)\n",
    "        \n",
    "        out = out + map_\n",
    "        \n",
    "        return x, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2943e217",
   "metadata": {},
   "source": [
    "## PAA-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed77fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PAA_d(nn.Module):\n",
    "    # dense decoder, it can be replaced by other decoder previous, such as DSS, amulet, and so on.\n",
    "    # used after MSF\n",
    "    def __init__(self, channel):\n",
    "        super(PAA_d, self).__init__()\n",
    "        self.conv1 = conv(channel * 3 ,channel, 3)\n",
    "        self.conv2 = conv(channel, channel, 3)\n",
    "        self.conv3 = conv(channel, channel, 3)\n",
    "        self.conv4 = conv(channel, channel, 3)\n",
    "        self.conv5 = conv(channel, 1, 3, bn=False)\n",
    "        \n",
    "        # PAA\n",
    "        self.Hattn = self_attn(channel, mode='h')\n",
    "        self.Wattn = self_attn(channel, mode='w')\n",
    "\n",
    "        self.upsample = lambda img, size: F.interpolate(img, size=size, mode='bilinear', align_corners=True)\n",
    "        \n",
    "    def forward(self, f1, f2, f3):\n",
    "        '''f1, f2, f3: PAA encoder features\n",
    "           f1: (N, 256, H/16, W/16), f2: (N, 256, H/16, W/16), f3: (N, 256, H/8, W/8)'''\n",
    "        # up-sampling\n",
    "        f1 = self.upsample(f1, f3.shape[-2:])\n",
    "        f2 = self.upsample(f2, f3.shape[-2:])\n",
    "        \n",
    "        # concatenation\n",
    "        f_con = torch.cat([f1, f2, f3], dim=1)\n",
    "        f_con = self.conv1(f_con)\n",
    "        \n",
    "        # PAA\n",
    "        Hf_con = self.Hattn(f_con)\n",
    "        Wf_con = self.Wattn(f_con)\n",
    "        f_con = Hf_con + Wf_con\n",
    "        \n",
    "        f_con = self.conv2(f_con)\n",
    "        f_con = self.conv3(f_con)\n",
    "        f_con = self.conv4(f_con)\n",
    "        \n",
    "        out = self.conv5(f_con)\n",
    "\n",
    "        return f_con, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14759b",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf5bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from backbones.Res2Net_v1b import res2net50_v1b_26w_4s\n",
    "from optim.losses import *\n",
    "\n",
    "\n",
    "class UACANet(nn.Module):\n",
    "    # res2net based encoder decoder\n",
    "    def __init__(self, channels=256, output_stride=16, pretrained=False):\n",
    "        super(UACANet, self).__init__()\n",
    "        self.resnet = res2net50_v1b_26w_4s(pretrained=pretrained, output_stride=output_stride)\n",
    "        \n",
    "        # parallel PAA encoders\n",
    "        self.context2 = PAA_e(512, channels)\n",
    "        self.context3 = PAA_e(1024, channels)\n",
    "        self.context4 = PAA_e(2048, channels)\n",
    "\n",
    "        self.decoder = PAA_d(channels)\n",
    "        \n",
    "        # parallel uncertainty augmented context attention\n",
    "        self.attention2 = UACA(channels * 2, channels)\n",
    "        self.attention3 = UACA(channels * 2, channels)\n",
    "        self.attention4 = UACA(channels * 2, channels)\n",
    "\n",
    "        self.loss_fn = bce_iou_loss\n",
    "\n",
    "        self.ret = lambda x, target: F.interpolate(x, size=target.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        self.res = lambda x, size: F.interpolate(x, size=size, mode='bilinear', align_corners=False)\n",
    "        \n",
    "\n",
    "    def forward(self, sample):\n",
    "        x = sample['image']  # (N, 3, H, W)\n",
    "        if 'gt' in sample.keys():\n",
    "            y = sample['gt']  # (N, 1, H, W)\n",
    "        else:\n",
    "            y = None\n",
    "            \n",
    "        base_size = x.shape[-2:]\n",
    "        \n",
    "        x = self.resnet.conv1(x)\n",
    "        x = self.resnet.bn1(x)\n",
    "        x = self.resnet.relu(x)\n",
    "        x = self.resnet.maxpool(x)  # (N, 64, H/4, W/4)\n",
    "        x1 = self.resnet.layer1(x)  # (N, 256, H/4, W/4)\n",
    "        \n",
    "        # ------------------ 1.Backbone -----------------------\n",
    "        # intermediate feature maps of backbone\n",
    "        x2 = self.resnet.layer2(x1)  # (N, 512, H/8, W/8)\n",
    "        x3 = self.resnet.layer3(x2)  # (N, 1024, H/16, W/16)\n",
    "        x4 = self.resnet.layer4(x3)  # (N, 2048, H/16, W/16)\n",
    "        \n",
    "        # ------------------ 2.PAA-Encoders -------------------\n",
    "        # PAA encoded features\n",
    "        x2_enc = self.context2(x2)  # (N, 256, H/8, W/8)\n",
    "        x3_enc = self.context3(x3)  # (N, 256, H/16, W/16)\n",
    "        x4_enc = self.context4(x4)  # (N, 256, H/16, W/16)\n",
    "        \n",
    "        # ------------------ 3.PAA-Decoders -------------------\n",
    "        # PAA decoding(initial prediction) - aggregates multi-scale PAA encoder features\n",
    "        # a5: initial saliency map for UACA\n",
    "        f5, a5 = self.decoder(x4_enc, x3_enc, x2_enc)  # (N, 256, H/8, W/8), (N, 1, H/8, W/8)\n",
    "        out5 = self.res(a5, base_size)  # (N, 1, H, W)\n",
    "        \n",
    "        # Sequential UACAs\n",
    "        # ------------------ 4.1 UACA -------------------\n",
    "        # inputs: concat-(N, 512, H/16, W/16), a5-(N, 1, H/8, W/8)\n",
    "        # f4: (N, 256, H/16, W/16), a4: (N, 1, H/16, W/16)\n",
    "        f4, a4 = self.attention4(torch.cat([x4_enc, self.ret(f5, x4_enc)], dim=1), a5)  \n",
    "        out4 = self.res(a4, base_size)  # (N, 1, H, W)\n",
    "        \n",
    "        # ------------------ 4.2 UACA -------------------\n",
    "        # inputs: concat-(N, 512, H/16, W/16), a4-(N, 1, H/16, W/16)\n",
    "        # f3: (N, 256, H/16, W/16), a3: (N, 1, H/16, W/16)\n",
    "        f3, a3 = self.attention3(torch.cat([x3_enc, self.ret(f4, x3_enc)], dim=1), a4)  \n",
    "        out3 = self.res(a3, base_size)  # (N, 1, H, W)\n",
    "\n",
    "        # ------------------ 4.3 UACA -------------------\n",
    "        # inputs: concat-(N, 512, H/8, W/8), a3-(N, 1, H/16, W/16)\n",
    "        # a2: (N, 1, H/8, W/8)\n",
    "        _, a2 = self.attention2(torch.cat([x2_enc, self.ret(f3, x2_enc)], dim=1), a3)  # (N, 1, H/8, W/8)\n",
    "        out2 = self.res(a2, base_size)  # (N, 1, H, W)\n",
    "        \n",
    "        if y is not None:\n",
    "            loss5 = self.loss_fn(out5, y)\n",
    "            loss4 = self.loss_fn(out4, y)\n",
    "            loss3 = self.loss_fn(out3, y)\n",
    "            loss2 = self.loss_fn(out2, y)\n",
    "\n",
    "            loss = loss2 + loss3 + loss4 + loss5\n",
    "            debug = [out5, out4, out3]\n",
    "            \n",
    "        else:\n",
    "            loss = 0\n",
    "            debug = []\n",
    "\n",
    "        return {'pred': out2, 'loss': loss, 'debug': debug}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51971525",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn([2, 3, 256, 256])\n",
    "y = torch.randn([2, 1, 256, 256])\n",
    "sample ={}\n",
    "sample['image'] = x\n",
    "sample['gt'] = y\n",
    "\n",
    "model = UACANet()\n",
    "out = model(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bebfb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c8b329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
