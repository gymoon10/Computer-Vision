{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MoCoV3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_PsFocM9T_v"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "hfg2QS4J9ZYV"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# utils\n",
        "@torch.no_grad()\n",
        "def concat_all_gather(tensor):\n",
        "    \"\"\"\n",
        "    Performs all_gather operation on the provided tensors.\n",
        "    *** Warning ***: torch.distributed.all_gather has no gradient.\n",
        "    \"\"\"\n",
        "    tensors_gather = [torch.ones_like(tensor)\n",
        "        for _ in range(torch.distributed.get_world_size())]\n",
        "    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)\n",
        "\n",
        "    output = torch.cat(tensors_gather, dim=0)\n",
        "    \n",
        "    return output"
      ],
      "metadata": {
        "id": "fYwPMOJBAkwH"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoCo(nn.Module):\n",
        "    \"\"\"\n",
        "    Build a MoCo model with a base encoder, a momentum encoder, and two MLPs (proj & pred )\n",
        "    https://arxiv.org/abs/1911.05722\n",
        "    \"\"\"\n",
        "    def __init__(self, base_encoder, dim=256, mlp_dim=4096, T=1.0):\n",
        "        \"\"\"\n",
        "        dim: feature dimension (default: 256)\n",
        "        mlp_dim: hidden dimension in MLPs (default: 4096)\n",
        "        T: softmax temperature (default: 1.0)\n",
        "        \"\"\"\n",
        "        super(MoCo, self).__init__()\n",
        "\n",
        "        self.T = T\n",
        "\n",
        "        # build encoders\n",
        "        self.base_encoder = base_encoder(num_classes=mlp_dim)  # f_q (backbone + proj mlp + pred mlp)\n",
        "        self.momentum_encoder = base_encoder(num_classes=mlp_dim)  # f_k (backbone + proj mlp)\n",
        "\n",
        "        self._build_projector_and_predictor_mlps(dim, mlp_dim)\n",
        "\n",
        "        for param_b, param_m in zip(self.base_encoder.parameters(), self.momentum_encoder.parameters()):\n",
        "            param_m.data.copy_(param_b.data)  # initialize (update by gradient)\n",
        "            param_m.requires_grad = False  # not update by gradient (momentum update)\n",
        "\n",
        "    def build_mlp(self, num_layers, input_dim, mlp_dim, output_dim, last_bn=True):\n",
        "        mlp = []\n",
        "        for l in range(num_layers):\n",
        "            dim1 = input_dim if l == 0 else mlp_dim  # 첫 번째 layer면 input_dim의 입력\n",
        "            dim2 = output_dim if l == num_layers - 1 else mlp_dim  # 마지막 layer면 output_dim의 출력\n",
        "            \n",
        "            # 첫 번째 layer : nn.Linear(input_dim, mlp_dim)\n",
        "            # 중간 layers : nn.Linear(mlp_dim, mlp_dim)\n",
        "            # 마지막 layer : nn.Linear(mlp_dim, output_dim)\n",
        "            mlp.append(nn.Linear(dim1, dim2, bias=False))\n",
        "\n",
        "            if l < num_layers - 1:  # 마지막 layer를 제외한 layer들에는 B.N, ReLU 적용\n",
        "                mlp.append(nn.BatchNorm1d(dim2))\n",
        "                mlp.append(nn.ReLU(inplace=True))\n",
        "\n",
        "            elif last_bn:  # 마지막 layer에는 B.N만\n",
        "                # follow SimCLR's design: https://github.com/google-research/simclr/blob/master/model_util.py#L157\n",
        "                # for simplicity, we further removed gamma in BN\n",
        "                mlp.append(nn.BatchNorm1d(dim2, affine=False))\n",
        "\n",
        "        return nn.Sequential(*mlp)\n",
        "\n",
        "    def _build_projector_and_predictor_mlps(self, dim, mlp_dim):\n",
        "        pass\n",
        "\n",
        "    @torch.no_grad()  # no gradient\n",
        "    def _update_momentum_encoder(self, m):\n",
        "        \"\"\"Momentum update of the momentum encoder\"\"\"\n",
        "        for param_b, param_m in zip(self.base_encoder.parameters(), self.momentum_encoder.parameters()):\n",
        "            param_m.data = param_m.data * m + param_b.data * (1. - m)\n",
        "\n",
        "    def contrastive_loss(self, q, k):\n",
        "        q = nn.functional.normalize(q, dim=1)\n",
        "        k = nn.functional.normalize(k, dim=1)\n",
        "        \n",
        "        # gather all targets\n",
        "        k = concat_all_gather(k)\n",
        "\n",
        "        # Einstein sum is more intuitive\n",
        "        logits = torch.einsum('nc, mc -> nm', [q, k]) / self.T  # [N, N] pairs\n",
        "        N = logits.shape[0]  # batch size per GPU\n",
        "\n",
        "        labels = (torch.arange(N, dtype=torch.long) + N * torch.distributed.get_rank()).cuda()  # positives are in diagonal\n",
        "\n",
        "        return nn.CrossEntropyLoss()(logits, labels) * (2 * self.T)\n",
        "\n",
        "    def forward(self, x1, x2, m):\n",
        "        \"\"\"\n",
        "        Input:\n",
        "            x1: first views of images by augmentation\n",
        "            x2: second views of images by different augmentation\n",
        "            m: moco momentum\n",
        "        Output:\n",
        "            loss\n",
        "        \"\"\"\n",
        "\n",
        "        # compute features ([N, C], C=256)\n",
        "        q1 = self.predictor(self.base_encoder(x1))  # query_encoder : backbone + proj mlp + pred mlp\n",
        "        q2 = self.predictor(self.base_encoder(x2))\n",
        "\n",
        "        with torch.no_grad():  # no gradient\n",
        "            self._update_momentum_encoder(m)  # update the momentum encoder\n",
        "\n",
        "            # compute momentum features as targets ([N, C], C=256)\n",
        "            k1 = self.momentum_encoder(x1)\n",
        "            k2 = self.momentum_encoder(x2)\n",
        "\n",
        "        return self.contrastive_loss(q1, k2) + self.contrastive_loss(q2, k1)  # symmetrized"
      ],
      "metadata": {
        "id": "tD9Wrg1Y9q7K"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MoCo_ResNet(MoCo):\n",
        "    def _build_projector_and_predictor_mlps(self, dim, mlp_dim):\n",
        "        hidden_dim = self.base_encoder.fc.weight.shape[1]\n",
        "        del self.base_encoder.fc, self.momentum_encoder.fc  # remove original fc layer\n",
        "\n",
        "        # projectors (f_q, f_k 모두 proj mlp를 가짐)\n",
        "        self.base_encoder.fc = self._build_mlp(2, hidden_dim, mlp_dim, dim)\n",
        "        self.momentum_encoder.fc = self._build_mlp(2, hidden_dim, mlp_dim, dim)\n",
        "\n",
        "        # predictor (f_q만 pred mlp를 가짐)\n",
        "        self.predictor = self._build_mlp(2, dim, mlp_dim, dim, False)\n",
        "\n",
        "\n",
        "class MoCo_ViT(MoCo):\n",
        "    def _build_projector_and_predictor_mlps(self, dim, mlp_dim):  # dim=256, mlp_dim=4096\n",
        "        hidden_dim = self.base_encoder.head.weight.shape[1]\n",
        "        del self.base_encoder.head, self.momentum_encoder.head  # remove original fc layer\n",
        "\n",
        "        # projectors (f_q, f_k 모두 proj mlp를 가짐)\n",
        "        self.base_encoder.head = self._build_mlp(3, hidden_dim, mlp_dim, dim)  # (num_layers, input_dim, mlp_dim, output_dim)\n",
        "        self.momentum_encoder.head = self._build_mlp(3, hidden_dim, mlp_dim, dim)\n",
        "\n",
        "        # predictor (f_q만 pred mlp를 가짐)\n",
        "        self.predictor = self._build_mlp(2, dim, mlp_dim, dim)"
      ],
      "metadata": {
        "id": "0QsCAjLb9w4D"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFeLvEOzVqyL",
        "outputId": "acfaade9-4ed7-44a2-8145-af8eb152d153"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.5.4)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial, reduce\n",
        "from operator import mul\n",
        "\n",
        "from timm.models.vision_transformer import VisionTransformer, _cfg\n",
        "from timm.models.layers.helpers import to_2tuple\n",
        "from timm.models.layers import PatchEmbed\n",
        "\n",
        "__all__ = [\n",
        "    'vit_small', \n",
        "    'vit_base',\n",
        "    'vit_conv_small',\n",
        "    'vit_conv_base',\n",
        "]\n",
        "\n",
        "\n",
        "class VisionTransformerMoCo(VisionTransformer):\n",
        "    def __init__(self, stop_grad_conv1=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        # Use fixed 2D sin-cos position embedding\n",
        "        self.build_2d_sincos_position_embedding()\n",
        "\n",
        "        # weight initialization\n",
        "        for name, m in self.named_modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                if 'qkv' in name:\n",
        "                    # treat the weights of Q, K, V separately\n",
        "                    val = math.sqrt(6. / float(m.weight.shape[0] // 3 + m.weight.shape[1]))\n",
        "                    nn.init.uniform_(m.weight, -val, val)\n",
        "                else:\n",
        "                    nn.init.xavier_uniform_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "        nn.init.normal_(self.cls_token, std=1e-6)\n",
        "\n",
        "        if isinstance(self.patch_embed, PatchEmbed):\n",
        "            # xavier_uniform initialization\n",
        "            val = math.sqrt(6. / float(3 * reduce(mul, self.patch_embed.patch_size, 1) + self.embed_dim))\n",
        "            nn.init.uniform_(self.patch_embed.proj.weight, -val, val)\n",
        "            nn.init.zeros_(self.patch_embed.proj.bias)\n",
        "\n",
        "            if stop_grad_conv1:\n",
        "                self.patch_embed.proj.weight.requires_grad = False  # proj layer 학습 x (학습의 안정성을 위해)\n",
        "                self.patch_embed.proj.bias.requires_grad = False\n",
        "\n",
        "    def build_2d_sincos_position_embedding(self, temperature=10000.):\n",
        "        h, w = self.patch_embed.grid_size\n",
        "        grid_w = torch.arange(w, dtype=torch.float32)\n",
        "        grid_h = torch.arange(h, dtype=torch.float32)\n",
        "        grid_w, grid_h = torch.meshgrid(grid_w, grid_h)\n",
        "        assert self.embed_dim % 4 == 0, 'Embed dimension must be divisible by 4 for 2D sin-cos position embedding'\n",
        "        pos_dim = self.embed_dim // 4\n",
        "        omega = torch.arange(pos_dim, dtype=torch.float32) / pos_dim\n",
        "        omega = 1. / (temperature**omega)\n",
        "        out_w = torch.einsum('m,d->md', [grid_w.flatten(), omega])\n",
        "        out_h = torch.einsum('m,d->md', [grid_h.flatten(), omega])\n",
        "        pos_emb = torch.cat([torch.sin(out_w), torch.cos(out_w), torch.sin(out_h), torch.cos(out_h)], dim=1)[None, :, :]\n",
        "\n",
        "        assert self.num_tokens == 1, 'Assuming one and only one token, [cls]'\n",
        "        pe_token = torch.zeros([1, 1, self.embed_dim], dtype=torch.float32)\n",
        "        self.pos_embed = nn.Parameter(torch.cat([pe_token, pos_emb], dim=1))\n",
        "        self.pos_embed.requires_grad = False  # 고정된 sin, cos함수에 의해 계산되는 값 사용\n",
        "\n",
        "\n",
        "class ConvStem(nn.Module):\n",
        "    \"\"\" \n",
        "    ConvStem, from Early Convolutions Help Transformers See Better, Tete et al. https://arxiv.org/abs/2106.14881\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, norm_layer=None, flatten=True):\n",
        "        super().__init__()\n",
        "\n",
        "        assert patch_size == 16, 'ConvStem only supports patch size of 16'\n",
        "        assert embed_dim % 8 == 0, 'Embed dimension must be divisible by 8 for ConvStem'\n",
        "\n",
        "        img_size = to_2tuple(img_size)\n",
        "        patch_size = to_2tuple(patch_size)\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
        "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
        "        self.flatten = flatten\n",
        "\n",
        "        # build stem, similar to the design in https://arxiv.org/abs/2106.14881\n",
        "        stem = []\n",
        "        input_dim, output_dim = 3, embed_dim // 8\n",
        "        for l in range(4):\n",
        "            stem.append(nn.Conv2d(input_dim, output_dim, kernel_size=3, stride=2, padding=1, bias=False))\n",
        "            stem.append(nn.BatchNorm2d(output_dim))\n",
        "            stem.append(nn.ReLU(inplace=True))\n",
        "            input_dim = output_dim\n",
        "            output_dim *= 2\n",
        "        stem.append(nn.Conv2d(input_dim, embed_dim, kernel_size=1))\n",
        "        self.proj = nn.Sequential(*stem)\n",
        "\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
        "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
        "        x = self.proj(x)\n",
        "        if self.flatten:\n",
        "            x = x.flatten(2).transpose(1, 2)  # BCHW -> BNC\n",
        "        x = self.norm(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "cT7vCSS8Nt6f"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = VisionTransformer()"
      ],
      "metadata": {
        "id": "3iqsHmX_VsdS"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.patch_embed  # proj layer의 가중치를 동결"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VspH41BnVw3r",
        "outputId": "703fdfa0-6cf4-4626-8e72-86f52bec6750"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PatchEmbed(\n",
              "  (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "  (norm): Identity()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit.pos_embed  # default로 sin-cos p.e를 사용하기 때문에 동결"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYSsnaN3Vya0",
        "outputId": "71d048c8-24d6-41f2-d69c-dedbf79fa69d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[-0.0074, -0.0156, -0.0019,  ..., -0.0114,  0.0447,  0.0409],\n",
              "         [-0.0138, -0.0257,  0.0037,  ...,  0.0070,  0.0142, -0.0061],\n",
              "         [-0.0388,  0.0253,  0.0033,  ...,  0.0076, -0.0044,  0.0165],\n",
              "         ...,\n",
              "         [ 0.0072, -0.0124,  0.0041,  ..., -0.0096,  0.0465,  0.0247],\n",
              "         [ 0.0320, -0.0056, -0.0067,  ...,  0.0102,  0.0334,  0.0168],\n",
              "         [-0.0079,  0.0006,  0.0090,  ..., -0.0131,  0.0335, -0.0109]]],\n",
              "       requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MbEf4Z-eWHqN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}