{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd3d31e0",
   "metadata": {},
   "source": [
    "https://github.com/Eromera/erfnet_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc078467",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d34b70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bbd47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTENSIONS = ['.jpg', '.png']\n",
    "\n",
    "def load_image(file):\n",
    "    return Image.open(file)\n",
    "\n",
    "def is_image(filename):\n",
    "    return any(filename.endswith(ext) for ext in EXTENSIONS)\n",
    "\n",
    "def is_label(filename):\n",
    "    return filename.endswith('labelTrainIds.png')\n",
    "\n",
    "# 이미지 파일 전체 경로 ex) 'E:/cityscapes/leftImg8bit/leftImg8bit/train\\\\aachen\\\\aachen_000000_000019_leftImg8bit.png'\n",
    "def image_path(root, basename, extension):\n",
    "    return os.path.join(root, f'{basename}{extension}')\n",
    "\n",
    "def image_path_city(root, name):\n",
    "    return os.path.join(root, f'{name}')\n",
    "\n",
    "# 이미지 파일 명 (앞의 경로는 제외하고) ex) 'aachen_000000_000019_leftImg8bit'\n",
    "def image_basename(filename):\n",
    "    return os.path.basename(os.path.splitext(filename)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91ff70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VOC12(Dataset):\n",
    "    def __init__(self, root, input_transform=None, target_transform=None):\n",
    "        self.images_root = os.path.join(root, 'images')\n",
    "        self.labels_root = os.path.join(root, 'labels')\n",
    "        \n",
    "        self.filenames = [image_basename(f) for f in os.listdir(self.labels_root) if is_image(f)]\n",
    "        self.filenames.sort()\n",
    "        \n",
    "        self.input_transform = input_transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        filename = self.filenames[index]\n",
    "        \n",
    "        with open(image_path(self.images_root, filename, '.jpg'), 'rb') as f:\n",
    "            image = load_image(f).convert('RGB')\n",
    "        with open(image_path(self.labels_root, filename, '.png'), 'rb') as f:\n",
    "            label = load_image(f).convert('P')\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cac0c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cityscapes(Dataset):\n",
    "\n",
    "    def __init__(self, root, co_transform=None, subset='train'):\n",
    "        self.images_root = os.path.join(root, 'leftImg8bit/')\n",
    "        self.labels_root = os.path.join(root, 'gtFine/')\n",
    "        \n",
    "        self.images_root += subset\n",
    "        self.labels_root += subset\n",
    "\n",
    "        print (self.images_root)\n",
    "        #self.filenames = [image_basename(f) for f in os.listdir(self.images_root) if is_image(f)]\n",
    "        self.filenames = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.images_root)) for f in fn if is_image(f)]\n",
    "        self.filenames.sort()  # train폴더의 모든 하위 폴더들에 있는 이미지 파일 목록\n",
    "\n",
    "        #[os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(\".\")) for f in fn]\n",
    "        #self.filenamesGt = [image_basename(f) for f in os.listdir(self.labels_root) if is_image(f)]\n",
    "        self.filenamesGt = [os.path.join(dp, f) for dp, dn, fn in os.walk(os.path.expanduser(self.labels_root)) for f in fn if is_label(f)]\n",
    "        self.filenamesGt.sort()\n",
    "\n",
    "        self.co_transform = co_transform # ADDED THIS\n",
    "    \n",
    "    # image, image에 대응되는 GT annotation 불러오기\n",
    "    def __getitem__(self, index):\n",
    "        filename = self.filenames[index]\n",
    "        filenameGt = self.filenamesGt[index]\n",
    "\n",
    "        with open(image_path_city(self.images_root, filename), 'rb') as f:\n",
    "            image = load_image(f).convert('RGB')\n",
    "        with open(image_path_city(self.labels_root, filenameGt), 'rb') as f:\n",
    "            label = load_image(f).convert('P')\n",
    "\n",
    "        if self.co_transform is not None:\n",
    "            image, label = self.co_transform(image, label)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5d1c0a",
   "metadata": {},
   "source": [
    "## 2. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663d25f",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/44194558/161220272-05733a2e-13cd-4301-b604-43a4ee127415.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f4cdf4",
   "metadata": {},
   "source": [
    "### 2.1 Factorized residual layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4853da2",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/44194558/161220125-ad4aa4dc-aaf7-4054-b1bb-863a40e47988.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6b0b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6283fd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'stacking sequentailly the proposed non-bt-1D layers'\n",
    "class non_bottleneck_1d (nn.Module):\n",
    "    def __init__(self, chann, dropprob, dilated):        \n",
    "        super().__init__()\n",
    "        \n",
    "        # 1 ~ 4 : facorized 3x3 conv filter (decomposition)\n",
    "        # 1. 3x1 filter\n",
    "        self.conv3x1_1 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1, 0), bias=True)\n",
    "        \n",
    "        # 2. 1x3 filter\n",
    "        self.conv1x3_1 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0, 1), bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(chann, eps=1e-03)\n",
    "        \n",
    "        # 3. 3x1 filter\n",
    "        self.conv3x1_2 = nn.Conv2d(chann, chann, (3, 1), stride=1, padding=(1*dilated,0), bias=True, dilation = (dilated, 1))\n",
    "        \n",
    "        # 4. 1x3 filter\n",
    "        self.conv1x3_2 = nn.Conv2d(chann, chann, (1, 3), stride=1, padding=(0,1*dilated), bias=True, dilation = (1, dilated))\n",
    "        self.bn2 = nn.BatchNorm2d(chann, eps=1e-03)\n",
    "        self.dropout = nn.Dropout2d(dropprob)\n",
    "\n",
    "    def forward(self, input):\n",
    "\n",
    "        output = self.conv3x1_1(input)\n",
    "        output = F.relu(output)\n",
    "        output = self.conv1x3_1(output)\n",
    "        output = self.bn1(output)\n",
    "        output = F.relu(output)\n",
    "\n",
    "        output = self.conv3x1_2(output)\n",
    "        output = F.relu(output)\n",
    "        output = self.conv1x3_2(output)\n",
    "        output = self.bn2(output)\n",
    "\n",
    "        if (self.dropout.p != 0):\n",
    "            output = self.dropout(output)\n",
    "        \n",
    "        return F.relu(output + input)  # +input = identity (residual connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40a6ba6",
   "metadata": {},
   "source": [
    "### 2.2 Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d513b10c",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/44194558/161219843-40a882cc-5151-460c-8e17-9986c5137de3.png)\n",
    "\n",
    "Encoder, decoder는 개별적으로 학습됨. 우선적으로 Encoder만을 개별적으로 학습하고, decoder를 붙여 전체 네트워크를 학습. \n",
    "\n",
    "Encoder 학습에 있어 2가지 전략 존재.\n",
    "\n",
    "<br/>\n",
    "\n",
    "1. From scratch\n",
    "\n",
    " - Encoder 마지막 단에 추가적인 conv layer를 붙이고, Cityscape만을 이용하여 학습.\n",
    " \n",
    " - Encoder 학습이 완료되면 마지막 conv layer를 삭제하고 decoder를 붙여 전체 네트워크를 학습\n",
    " \n",
    "<br/> \n",
    " \n",
    "2. Pretrained\n",
    "\n",
    " - Encoder의 마지막 layer를 적용시켜 single classification output 출력.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ede8e3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'performs down-sampling by concatenating the parallel outputs of a single 3x3 conv(stride=2) and a Maxpooling' (for layer1, 2, 8)\n",
    "class DownsamplerBlock (nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Conv2d(n_input, n_output-n_input, (3, 3), stride=2, padding=1, bias=True)\n",
    "        self.pool = nn.MaxPool2d(2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(n_output, eps=1e-3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # concat outputs of conv & pooling modules\n",
    "        output = torch.cat([self.conv(input), self.pool(input)], 1)  # (n_out - n_in) + n_in\n",
    "        output = self.bn(output)\n",
    "        \n",
    "        return F.relu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26bd20c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer 1\n",
    "        self.initial_block = DownsamplerBlock(3, 16)\n",
    "        \n",
    "        # Layer 2 + Layer 3-7 \n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(DownsamplerBlock(16, 64))  # layer 2\n",
    "        \n",
    "        for x in range(0, 5):  # 5 x Non-bottleneck-1D  \n",
    "            self.layers.append(non_bottleneck_1d(64, 0.03, 1)) \n",
    "        \n",
    "        # Layer 8\n",
    "        self.layers.append(DownsamplerBlock(64, 128))\n",
    "        \n",
    "        # Layer 9 ~ 16\n",
    "        for x in range(0, 2):  # 2 times\n",
    "            self.layers.append(non_bottleneck_1d(128, 0.3, 2))\n",
    "            self.layers.append(non_bottleneck_1d(128, 0.3, 4))\n",
    "            self.layers.append(non_bottleneck_1d(128, 0.3, 8))\n",
    "            self.layers.append(non_bottleneck_1d(128, 0.3, 16))\n",
    "\n",
    "        # Only in encoder mode (predict=True)\n",
    "        self.output_conv = nn.Conv2d(128, num_classes, 1, stride=1, padding=0, bias=True)\n",
    "        \n",
    "        \n",
    "    def forward(self, input, predict=False):\n",
    "        output = self.initial_block(input)\n",
    "\n",
    "        for layer in self.layers:  \n",
    "            output = layer(output)\n",
    "\n",
    "        if predict:\n",
    "            output = self.output_conv(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94e8426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpsamplerBlock (nn.Module):\n",
    "    def __init__(self, n_input, n_output):\n",
    "        super().__init__()\n",
    "        # Deconv\n",
    "        self.conv = nn.ConvTranspose2d(n_input, n_output, 3, stride=2, padding=1, output_padding=1, bias=True)\n",
    "        self.bn = nn.BatchNorm2d(n_output, eps=1e-3)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv(input)\n",
    "        output = self.bn(output)\n",
    "        \n",
    "        return F.relu(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8a6e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder (nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "\n",
    "        self.layers.append(UpsamplerBlock(128, 64))\n",
    "        self.layers.append(non_bottleneck_1d(64, 0, 1))\n",
    "        self.layers.append(non_bottleneck_1d(64, 0, 1))\n",
    "\n",
    "        self.layers.append(UpsamplerBlock(64, 16))\n",
    "        self.layers.append(non_bottleneck_1d(16, 0, 1))\n",
    "        self.layers.append(non_bottleneck_1d(16, 0, 1))\n",
    "\n",
    "        self.output_conv = nn.ConvTranspose2d(16, num_classes, 2, stride=2, padding=0, output_padding=0, bias=True)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = input\n",
    "\n",
    "        for layer in self.layers:\n",
    "            output = layer(output)\n",
    "\n",
    "        output = self.output_conv(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10eebea",
   "metadata": {},
   "source": [
    "### 2.3 ERFNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c831fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes, encoder=None):  # use encoder to pass pretrained encoder\n",
    "        super().__init__()\n",
    "\n",
    "        if (encoder == None):\n",
    "            self.encoder = Encoder(num_classes)\n",
    "        else:\n",
    "            self.encoder = encoder\n",
    "            \n",
    "        self.decoder = Decoder(num_classes)\n",
    "\n",
    "    def forward(self, input, only_encode=False):\n",
    "        if only_encode:\n",
    "            return self.encoder.forward(input, predict=True)\n",
    "        \n",
    "        else:\n",
    "            output = self.encoder(input)    # predict=False by default\n",
    "            \n",
    "            return self.decoder.forward(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072e228b",
   "metadata": {},
   "source": [
    "## 3. Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb52614",
   "metadata": {},
   "source": [
    "![image](https://user-images.githubusercontent.com/44194558/161227213-f08c1fc7-210f-498b-97f0-dce600427acf.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1d87805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# pixel loss\n",
    "class iouEval:\n",
    "\n",
    "    def __init__(self, nClasses, ignoreIndex=19):\n",
    "        self.nClasses = nClasses  # PASCAL=22\n",
    "        self.ignoreIndex = ignoreIndex if nClasses > ignoreIndex else -1 #if ignoreIndex is larger than nClasses, consider no ignoreIndex\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        classes = self.nClasses if self.ignoreIndex ==-1 else self.nClasses-1\n",
    "        self.tp = torch.zeros(classes).double()\n",
    "        self.fp = torch.zeros(classes).double()\n",
    "        self.fn = torch.zeros(classes).double()        \n",
    "\n",
    "    def addBatch(self, x, y):  \n",
    "        \n",
    "        ''' x, y는 아래와 같은 형태여야 함\n",
    "           x : prediction (N, nClasses, H, W)\n",
    "           y : GT (N, nClasses, H, W)\n",
    "        '''\n",
    "        #print (\"X is cuda: \", x.is_cuda)\n",
    "        #print (\"Y is cuda: \", y.is_cuda)\n",
    "\n",
    "        if (x.is_cuda or y.is_cuda):\n",
    "            x = x.cuda()\n",
    "            y = y.cuda()\n",
    "\n",
    "        # if size = (N, 1, H, W) -> one hot encoding\n",
    "        if (x.size(1) == 1):\n",
    "            x_onehot = torch.zeros(x.size(0), self.nClasses, x.size(2), x.size(3))  \n",
    "            if x.is_cuda:\n",
    "                x_onehot = x_onehot.cuda()\n",
    "            x_onehot.scatter_(1, x, 1).float()\n",
    "        else:\n",
    "            x_onehot = x.float()\n",
    "        \n",
    "        # if size = (N, 1, H, W) -> one hot encoding\n",
    "        if (y.size(1) == 1):\n",
    "            y_onehot = torch.zeros(y.size(0), self.nClasses, y.size(2), y.size(3))\n",
    "            if y.is_cuda:\n",
    "                y_onehot = y_onehot.cuda()\n",
    "            y_onehot.scatter_(1, y, 1).float()\n",
    "        else:\n",
    "            y_onehot = y.float()\n",
    "\n",
    "        if (self.ignoreIndex != -1): \n",
    "            ignores = y_onehot[:, self.ignoreIndex].unsqueeze(1)\n",
    "            x_onehot = x_onehot[:, :self.ignoreIndex]\n",
    "            y_onehot = y_onehot[:, :self.ignoreIndex]\n",
    "        else:\n",
    "            ignores=0\n",
    "\n",
    "        #print(type(x_onehot))\n",
    "        #print(type(y_onehot))\n",
    "        #print(x_onehot.size())\n",
    "        #print(y_onehot.size())\n",
    "        \n",
    "        # TP, FP, FN - 개별 class에 대해 pixel 단위로 count\n",
    "        tpmult = x_onehot * y_onehot    # times prediction and gt coincide is 1\n",
    "        tp = torch.sum(torch.sum(torch.sum(tpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n",
    "        \n",
    "        fpmult = x_onehot * (1-y_onehot-ignores) # times prediction says its that class and gt says its not (subtracting cases when its ignore label!)\n",
    "        fp = torch.sum(torch.sum(torch.sum(fpmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze()\n",
    "        \n",
    "        fnmult = (1-x_onehot) * (y_onehot) # times prediction says its not that class and gt says it is\n",
    "        fn = torch.sum(torch.sum(torch.sum(fnmult, dim=0, keepdim=True), dim=2, keepdim=True), dim=3, keepdim=True).squeeze() \n",
    "\n",
    "        self.tp += tp.double().cpu()\n",
    "        self.fp += fp.double().cpu()\n",
    "        self.fn += fn.double().cpu()\n",
    "\n",
    "    def getIoU(self):\n",
    "        num = self.tp\n",
    "        den = self.tp + self.fp + self.fn + 1e-15\n",
    "        iou = num / den\n",
    "        \n",
    "        return torch.mean(iou), iou     #returns \"iou mean\", \"iou per class\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "107ccac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification loss\n",
    "class CrossEntropyLoss2d(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = torch.nn.NLLLoss2d(weight)\n",
    "\n",
    "    def forward(self, outputs, targets):\n",
    "        return self.loss(torch.nn.functional.log_softmax(outputs, dim=1), targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cfcb77",
   "metadata": {},
   "source": [
    "## 4. Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3c26e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for colors\n",
    "class colors:\n",
    "    RED       = '\\033[31;1m'\n",
    "    GREEN     = '\\033[32;1m'\n",
    "    YELLOW    = '\\033[33;1m'\n",
    "    BLUE      = '\\033[34;1m'\n",
    "    MAGENTA   = '\\033[35;1m'\n",
    "    CYAN      = '\\033[36;1m'\n",
    "    BOLD      = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    ENDC      = '\\033[0m'\n",
    "\n",
    "# Colored value output if colorized flag is activated.\n",
    "def getColorEntry(val):\n",
    "    if not isinstance(val, float):\n",
    "        return colors.ENDC\n",
    "    if (val < .20):\n",
    "        return colors.RED\n",
    "    elif (val < .40):\n",
    "        return colors.YELLOW\n",
    "    elif (val < .60):\n",
    "        return colors.BLUE\n",
    "    elif (val < .80):\n",
    "        return colors.CYAN\n",
    "    else:\n",
    "        return colors.GREEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a686bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "def colormap_cityscapes(n):\n",
    "    cmap=np.zeros([n, 3]).astype(np.uint8)\n",
    "    cmap[0,:] = np.array([128, 64,128])\n",
    "    cmap[1,:] = np.array([244, 35,232])\n",
    "    cmap[2,:] = np.array([ 70, 70, 70])\n",
    "    cmap[3,:] = np.array([ 102,102,156])\n",
    "    cmap[4,:] = np.array([ 190,153,153])\n",
    "    cmap[5,:] = np.array([ 153,153,153])\n",
    "\n",
    "    cmap[6,:] = np.array([ 250,170, 30])\n",
    "    cmap[7,:] = np.array([ 220,220,  0])\n",
    "    cmap[8,:] = np.array([ 107,142, 35])\n",
    "    cmap[9,:] = np.array([ 152,251,152])\n",
    "    cmap[10,:] = np.array([ 70,130,180])\n",
    "\n",
    "    cmap[11,:] = np.array([ 220, 20, 60])\n",
    "    cmap[12,:] = np.array([ 255,  0,  0])\n",
    "    cmap[13,:] = np.array([ 0,  0,142])\n",
    "    cmap[14,:] = np.array([  0,  0, 70])\n",
    "    cmap[15,:] = np.array([  0, 60,100])\n",
    "\n",
    "    cmap[16,:] = np.array([  0, 80,100])\n",
    "    cmap[17,:] = np.array([  0,  0,230])\n",
    "    cmap[18,:] = np.array([ 119, 11, 32])\n",
    "    cmap[19,:] = np.array([ 0,  0,  0])\n",
    "    \n",
    "    return cmap\n",
    "\n",
    "\n",
    "def colormap(n):\n",
    "    cmap=np.zeros([n, 3]).astype(np.uint8)\n",
    "\n",
    "    for i in np.arange(n):\n",
    "        r, g, b = np.zeros(3)\n",
    "\n",
    "        for j in np.arange(8):\n",
    "            r = r + (1<<(7-j))*((i&(1<<(3*j))) >> (3*j))\n",
    "            g = g + (1<<(7-j))*((i&(1<<(3*j+1))) >> (3*j+1))\n",
    "            b = b + (1<<(7-j))*((i&(1<<(3*j+2))) >> (3*j+2))\n",
    "\n",
    "        cmap[i,:] = np.array([r, g, b])\n",
    "\n",
    "    return cmap\n",
    "\n",
    "class Relabel:\n",
    "\n",
    "    def __init__(self, olabel, nlabel):\n",
    "        self.olabel = olabel\n",
    "        self.nlabel = nlabel\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        assert (isinstance(tensor, torch.LongTensor) or isinstance(tensor, torch.ByteTensor)) , 'tensor needs to be LongTensor'\n",
    "        tensor[tensor == self.olabel] = self.nlabel\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class ToLabel:\n",
    "\n",
    "    def __call__(self, image):\n",
    "        return torch.from_numpy(np.array(image)).long().unsqueeze(0)\n",
    "\n",
    "\n",
    "class Colorize:\n",
    "\n",
    "    def __init__(self, n=22):\n",
    "        #self.cmap = colormap(256)\n",
    "        self.cmap = colormap_cityscapes(256)\n",
    "        self.cmap[n] = self.cmap[-1]\n",
    "        self.cmap = torch.from_numpy(self.cmap[:n])\n",
    "\n",
    "    def __call__(self, gray_image):\n",
    "        size = gray_image.size()\n",
    "        #print(size)\n",
    "        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n",
    "        #color_image = torch.ByteTensor(3, size[0], size[1]).fill_(0)\n",
    "\n",
    "        #for label in range(1, len(self.cmap)):\n",
    "        for label in range(0, len(self.cmap)):\n",
    "            mask = gray_image[0] == label\n",
    "            #mask = gray_image == label\n",
    "\n",
    "            color_image[0][mask] = self.cmap[label][0]\n",
    "            color_image[1][mask] = self.cmap[label][1]\n",
    "            color_image[2][mask] = self.cmap[label][2]\n",
    "\n",
    "        return color_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd1e95fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Augmentations - different function implemented to perform random augments on both image and target\n",
    "class MyCoTransform(object):\n",
    "    def __init__(self, enc, augment=True, height=512):\n",
    "        self.enc=enc\n",
    "        self.augment = augment\n",
    "        self.height = height\n",
    "        pass\n",
    "    def __call__(self, input, target):\n",
    "        # do something to both images\n",
    "        input =  Resize(self.height, Image.BILINEAR)(input)\n",
    "        target = Resize(self.height, Image.NEAREST)(target)\n",
    "\n",
    "        if(self.augment):\n",
    "            # Random hflip\n",
    "            hflip = random.random()\n",
    "            if (hflip < 0.5):\n",
    "                input = input.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                target = target.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "            \n",
    "            #Random translation 0-2 pixels (fill rest with padding\n",
    "            transX = random.randint(-2, 2) \n",
    "            transY = random.randint(-2, 2)\n",
    "\n",
    "            input = ImageOps.expand(input, border=(transX,transY,0,0), fill=0)\n",
    "            target = ImageOps.expand(target, border=(transX,transY,0,0), fill=255) #pad label filling with 255\n",
    "            input = input.crop((0, 0, input.size[0]-transX, input.size[1]-transY))\n",
    "            target = target.crop((0, 0, target.size[0]-transX, target.size[1]-transY))   \n",
    "\n",
    "        input = ToTensor()(input)\n",
    "        if (self.enc):\n",
    "            target = Resize(int(self.height/8), Image.NEAREST)(target)\n",
    "        target = ToLabel()(target)\n",
    "        target = Relabel(255, 19)(target)\n",
    "\n",
    "        return input, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d1168f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from visdom import Visdom\n",
    "\n",
    "class Dashboard:\n",
    "\n",
    "    def __init__(self, port):\n",
    "        self.vis = Visdom(port=port)\n",
    "\n",
    "    def loss(self, losses, title):\n",
    "        x = np.arange(1, len(losses)+1, 1)\n",
    "\n",
    "        self.vis.line(losses, x, env='loss', opts=dict(title=title))\n",
    "\n",
    "    def image(self, image, title):\n",
    "        if image.is_cuda:\n",
    "            image = image.cpu()\n",
    "        if isinstance(image, Variable):\n",
    "            image = image.data\n",
    "        image = image.numpy()\n",
    "\n",
    "        self.vis.image(image, env='images', opts=dict(title=title))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b3ee3",
   "metadata": {},
   "source": [
    "## 5. Main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4add9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import warnings\n",
    "\n",
    "from PIL import Image, ImageOps\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from torch.optim import SGD, Adam, lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize, Pad\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "\n",
    "import importlib\n",
    "\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7f90b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 20  # pascal=22, cityscapes=20\n",
    "\n",
    "color_transform = Colorize(NUM_CLASSES)\n",
    "image_transform = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9bc80df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "args = Namespace()\n",
    "args.cuda = True\n",
    "args.model = 'erfnet'  \n",
    "args.state = None\n",
    "args.port = 8097\n",
    "args.datadir = 'E:/cityscapes'\n",
    "args.height = 512\n",
    "args.num_epochs = 60\n",
    "args.num_workers = 0\n",
    "args.batch_size = 3\n",
    "args.steps_loss = 50\n",
    "args.steps_plot = 50\n",
    "args.epochs_save = 10\n",
    "args.savedir = 'E:/cityscapes/save/'\n",
    "args.decoder = 'store_true'\n",
    "args.pretrainedEncoder = None\n",
    "args.visualize = True\n",
    "\n",
    "args.ioutrain = False\n",
    "args.iouVal = True\n",
    "args.resume = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d9ac8575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, enc=False):\n",
    "    best_acc = 0\n",
    "    \n",
    "    weight = torch.ones(NUM_CLASSES)\n",
    "    enc = False  # default=False\n",
    "\n",
    "    # 각 class별로 수동으로 주어진 조정 가중치 (NLLLoss2d계산에 사용)\n",
    "    if (enc):\n",
    "        weight[0] = 2.3653597831726\n",
    "        weight[1] = 4.4237880706787\n",
    "        weight[2] = 2.9691488742828\n",
    "        weight[3] = 5.3442072868347\n",
    "        weight[4] = 5.2983593940735\n",
    "        weight[5] = 5.2275490760803\n",
    "        weight[6] = 5.4394111633301\n",
    "        weight[7] = 5.3659925460815\n",
    "        weight[8] = 3.4170460700989\n",
    "        weight[9] = 5.2414722442627\n",
    "        weight[10] = 4.7376127243042\n",
    "        weight[11] = 5.2286224365234\n",
    "        weight[12] = 5.455126285553\n",
    "        weight[13] = 4.3019247055054\n",
    "        weight[14] = 5.4264230728149\n",
    "        weight[15] = 5.4331531524658\n",
    "        weight[16] = 5.433765411377\n",
    "        weight[17] = 5.4631009101868\n",
    "        weight[18] = 5.3947434425354\n",
    "    \n",
    "    else:\n",
    "        weight[0] = 2.8149201869965\n",
    "        weight[1] = 6.9850029945374\n",
    "        weight[2] = 3.7890393733978\n",
    "        weight[3] = 9.9428062438965\n",
    "        weight[4] = 9.7702074050903\n",
    "        weight[5] = 9.5110931396484\n",
    "        weight[6] = 10.311357498169\n",
    "        weight[7] = 10.026463508606\n",
    "        weight[8] = 4.6323022842407\n",
    "        weight[9] = 9.5608062744141\n",
    "        weight[10] = 7.8698215484619\n",
    "        weight[11] = 9.5168733596802\n",
    "        weight[12] = 10.373730659485\n",
    "        weight[13] = 6.6616044044495\n",
    "        weight[14] = 10.260489463806\n",
    "        weight[15] = 10.287888526917\n",
    "        weight[16] = 10.289801597595\n",
    "        weight[17] = 10.405355453491\n",
    "        weight[18] = 10.138095855713\n",
    "\n",
    "    weight[19] = 0\n",
    "    \n",
    "    # Data\n",
    "    co_transform = MyCoTransform(enc, augment=True, height=args.height)  # 1024)\n",
    "    co_transform_val = MyCoTransform(enc, augment=False, height=args.height)  # 1024)\n",
    "    \n",
    "    dataset_train = cityscapes(args.datadir, co_transform, 'train')\n",
    "    dataset_val = cityscapes(args.datadir, co_transform_val, 'val')\n",
    "    \n",
    "    loader = DataLoader(dataset_train, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=True)\n",
    "    loader_val = DataLoader(dataset_val, num_workers=args.num_workers, batch_size=args.batch_size, shuffle=False)\n",
    "   \n",
    "    if args.cuda:\n",
    "        weight = weight.cuda()\n",
    "    # Loss\n",
    "    criterion = CrossEntropyLoss2d(weight)\n",
    "    print(type(criterion))\n",
    "    \n",
    "    savedir = f'{args.savedir}'\n",
    "    \n",
    "    if (enc):\n",
    "        automated_log_path = savedir + \"/automated_log_encoder.txt\"\n",
    "        modeltxtpath = savedir + \"/model_encoder.txt\"\n",
    "    else:\n",
    "        automated_log_path = savedir + \"/automated_log.txt\"\n",
    "        modeltxtpath = savedir + \"/model.txt\"    \n",
    "    \n",
    "    if (not os.path.exists(automated_log_path)):    #dont add first line if it exists \n",
    "        with open(automated_log_path, \"a\") as myfile:\n",
    "            myfile.write(\"Epoch\\t\\tTrain-loss\\t\\tTest-loss\\t\\tTrain-IoU\\t\\tTest-IoU\\t\\tlearningRate\")\n",
    "\n",
    "    with open(modeltxtpath, \"w\") as myfile:\n",
    "        myfile.write(str(model))\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = Adam(model.parameters(), 5e-4, (0.9, 0.999),  eps=1e-08, weight_decay=1e-4)   \n",
    "    \n",
    "    start_epoch = 1\n",
    "    if args.resume:\n",
    "        # Must load weights, optimizer, epoch and best value. \n",
    "        if enc:\n",
    "            filenameCheckpoint = savedir + '/checkpoint_enc.pth.tar'\n",
    "        else:\n",
    "            filenameCheckpoint = savedir + '/checkpoint.pth.tar'\n",
    "            \n",
    "    assert os.path.exists(filenameCheckpoint), \"Error: resume option was used but checkpoint was not found in folder\"\n",
    "    checkpoint = torch.load(filenameCheckpoint)\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "    best_acc = checkpoint['best_acc']\n",
    "    print(\"=> Loaded checkpoint at epoch {})\".format(checkpoint['epoch']))\n",
    "    \n",
    "    lambda1 = lambda epoch: pow((1-((epoch-1)/args.num_epochs)),0.9)  ## scheduler 2\n",
    "    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)                             ## scheduler 2\n",
    "\n",
    "    if args.visualize and args.steps_plot > 0:\n",
    "        board = Dashboard(args.port)\n",
    "        \n",
    "    for epoch in range(start_epoch, args.num_epochs+1):\n",
    "        print(\"----- TRAINING - EPOCH\", epoch, \"-----\")\n",
    "\n",
    "        scheduler.step(epoch)    ## scheduler 2\n",
    "\n",
    "        epoch_loss = []\n",
    "        time_train = []\n",
    "     \n",
    "        doIouTrain = args.iouTrain   \n",
    "        doIouVal =  args.iouVal      \n",
    "\n",
    "        if (doIouTrain):\n",
    "            iouEvalTrain = iouEval(NUM_CLASSES)\n",
    "\n",
    "        usedLr = 0\n",
    "        for param_group in optimizer.param_groups:\n",
    "            print(\"LEARNING RATE: \", param_group['lr'])\n",
    "            usedLr = float(param_group['lr'])\n",
    "        \n",
    "        # Train\n",
    "        model.train()\n",
    "        for step, (images, labels) in enumerate(loader):\n",
    "\n",
    "            start_time = time.time()\n",
    "            #print (labels.size())\n",
    "            #print (np.unique(labels.numpy()))\n",
    "            #print(\"labels: \", np.unique(labels[0].numpy()))\n",
    "            #labels = torch.ones(4, 1, 512, 1024).long()\n",
    "            if args.cuda:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            inputs = Variable(images)\n",
    "            targets = Variable(labels)\n",
    "            outputs = model(inputs, only_encode=enc)\n",
    "\n",
    "            #print(\"targets\", np.unique(targets[:, 0].cpu().data.numpy()))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(outputs, targets[:, 0])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.item())\n",
    "            time_train.append(time.time() - start_time)\n",
    "\n",
    "            if (doIouTrain):\n",
    "                #start_time_iou = time.time()\n",
    "                iouEvalTrain.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\n",
    "                #print (\"Time to add confusion matrix: \", time.time() - start_time_iou)      \n",
    "\n",
    "            #print(outputs.size())\n",
    "            if args.visualize and args.steps_plot > 0 and step % args.steps_plot == 0:\n",
    "                start_time_plot = time.time()\n",
    "                image = inputs[0].cpu().data\n",
    "                #image[0] = image[0] * .229 + .485\n",
    "                #image[1] = image[1] * .224 + .456\n",
    "                #image[2] = image[2] * .225 + .406\n",
    "                #print(\"output\", np.unique(outputs[0].cpu().max(0)[1].data.numpy()))\n",
    "                board.image(image, f'input (epoch: {epoch}, step: {step})')\n",
    "                if isinstance(outputs, list):   #merge gpu tensors\n",
    "                    board.image(color_transform(outputs[0][0].cpu().max(0)[1].data.unsqueeze(0)),\n",
    "                    f'output (epoch: {epoch}, step: {step})')\n",
    "                else:\n",
    "                    board.image(color_transform(outputs[0].cpu().max(0)[1].data.unsqueeze(0)),\n",
    "                    f'output (epoch: {epoch}, step: {step})')\n",
    "                board.image(color_transform(targets[0].cpu().data),\n",
    "                    f'target (epoch: {epoch}, step: {step})')\n",
    "                print (\"Time to paint images: \", time.time() - start_time_plot)\n",
    "            if args.steps_loss > 0 and step % args.steps_loss == 0:\n",
    "                average = sum(epoch_loss) / len(epoch_loss)\n",
    "                print(f'loss: {average:0.4} (epoch: {epoch}, step: {step})', \n",
    "                        \"// Avg time/img: %.4f s\" % (sum(time_train) / len(time_train) / args.batch_size))\n",
    "\n",
    "            \n",
    "        average_epoch_loss_train = sum(epoch_loss) / len(epoch_loss)\n",
    "        \n",
    "        iouTrain = 0\n",
    "        if (doIouTrain):\n",
    "            iouTrain, iou_classes = iouEvalTrain.getIoU()\n",
    "            iouStr = getColorEntry(iouTrain)+'{:0.2f}'.format(iouTrain*100) + '\\033[0m'\n",
    "            print (\"EPOCH IoU on TRAIN set: \", iouStr, \"%\")  \n",
    "\n",
    "        #Validate on 500 val images after each epoch of training\n",
    "        print(\"----- VALIDATING - EPOCH\", epoch, \"-----\")\n",
    "        model.eval()\n",
    "        epoch_loss_val = []\n",
    "        time_val = []\n",
    "        \n",
    "        if (doIouVal):\n",
    "            iouEvalVal = iouEval(NUM_CLASSES)\n",
    "\n",
    "        for step, (images, labels) in enumerate(loader_val):\n",
    "            start_time = time.time()\n",
    "            if args.cuda:\n",
    "                images = images.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            inputs = Variable(images, volatile=True)    #volatile flag makes it free backward or outputs for eval\n",
    "            targets = Variable(labels, volatile=True)\n",
    "            outputs = model(inputs, only_encode=enc) \n",
    "\n",
    "            loss = criterion(outputs, targets[:, 0])\n",
    "            epoch_loss_val.append(loss.item())\n",
    "            time_val.append(time.time() - start_time)\n",
    "\n",
    "\n",
    "            #Add batch to calculate TP, FP and FN for iou estimation\n",
    "            if (doIouVal):\n",
    "                #start_time_iou = time.time()\n",
    "                iouEvalVal.addBatch(outputs.max(1)[1].unsqueeze(1).data, targets.data)\n",
    "                #print (\"Time to add confusion matrix: \", time.time() - start_time_iou)\n",
    "\n",
    "            if args.visualize and args.steps_plot > 0 and step % args.steps_plot == 0:\n",
    "                start_time_plot = time.time()\n",
    "                image = inputs[0].cpu().data\n",
    "                board.image(image, f'VAL input (epoch: {epoch}, step: {step})')\n",
    "                if isinstance(outputs, list):   #merge gpu tensors\n",
    "                    board.image(color_transform(outputs[0][0].cpu().max(0)[1].data.unsqueeze(0)),\n",
    "                    f'VAL output (epoch: {epoch}, step: {step})')\n",
    "                else:\n",
    "                    board.image(color_transform(outputs[0].cpu().max(0)[1].data.unsqueeze(0)),\n",
    "                    f'VAL output (epoch: {epoch}, step: {step})')\n",
    "                board.image(color_transform(targets[0].cpu().data),\n",
    "                    f'VAL target (epoch: {epoch}, step: {step})')\n",
    "                print (\"Time to paint images: \", time.time() - start_time_plot)\n",
    "            if args.steps_loss > 0 and step % args.steps_loss == 0:\n",
    "                average = sum(epoch_loss_val) / len(epoch_loss_val)\n",
    "                print(f'VAL loss: {average:0.4} (epoch: {epoch}, step: {step})', \n",
    "                        \"// Avg time/img: %.4f s\" % (sum(time_val) / len(time_val) / args.batch_size))\n",
    "                       \n",
    "\n",
    "        average_epoch_loss_val = sum(epoch_loss_val) / len(epoch_loss_val)\n",
    "        #scheduler.step(average_epoch_loss_val, epoch)  ## scheduler 1   # update lr if needed\n",
    "\n",
    "        iouVal = 0\n",
    "        if (doIouVal):\n",
    "            iouVal, iou_classes = iouEvalVal.getIoU()\n",
    "            iouStr = getColorEntry(iouVal)+'{:0.2f}'.format(iouVal*100) + '\\033[0m'\n",
    "            print (\"EPOCH IoU on VAL set: \", iouStr, \"%\") \n",
    "           \n",
    "\n",
    "        # remember best valIoU and save checkpoint\n",
    "        if iouVal == 0:\n",
    "            current_acc = -average_epoch_loss_val\n",
    "        else:\n",
    "            current_acc = iouVal \n",
    "        is_best = current_acc > best_acc\n",
    "        best_acc = max(current_acc, best_acc)\n",
    "        if enc:\n",
    "            filenameCheckpoint = savedir + '/checkpoint_enc.pth.tar'\n",
    "            filenameBest = savedir + '/model_best_enc.pth.tar'    \n",
    "        else:\n",
    "            filenameCheckpoint = savedir + '/checkpoint.pth.tar'\n",
    "            filenameBest = savedir + '/model_best.pth.tar'\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'arch': str(model),\n",
    "            'state_dict': model.state_dict(),\n",
    "            'best_acc': best_acc,\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, is_best, filenameCheckpoint, filenameBest)\n",
    "\n",
    "        #SAVE MODEL AFTER EPOCH\n",
    "        if (enc):\n",
    "            filename = f'{savedir}/model_encoder-{epoch:03}.pth'\n",
    "            filenamebest = f'{savedir}/model_encoder_best.pth'\n",
    "        else:\n",
    "            filename = f'{savedir}/model-{epoch:03}.pth'\n",
    "            filenamebest = f'{savedir}/model_best.pth'\n",
    "        if args.epochs_save > 0 and step > 0 and step % args.epochs_save == 0:\n",
    "            torch.save(model.state_dict(), filename)\n",
    "            print(f'save: {filename} (epoch: {epoch})')\n",
    "        if (is_best):\n",
    "            torch.save(model.state_dict(), filenamebest)\n",
    "            print(f'save: {filenamebest} (epoch: {epoch})')\n",
    "            if (not enc):\n",
    "                with open(savedir + \"/best.txt\", \"w\") as myfile:\n",
    "                    myfile.write(\"Best epoch is %d, with Val-IoU= %.4f\" % (epoch, iouVal))   \n",
    "            else:\n",
    "                with open(savedir + \"/best_encoder.txt\", \"w\") as myfile:\n",
    "                    myfile.write(\"Best epoch is %d, with Val-IoU= %.4f\" % (epoch, iouVal))           \n",
    "\n",
    "        #SAVE TO FILE A ROW WITH THE EPOCH RESULT (train loss, val loss, train IoU, val IoU)\n",
    "        #Epoch\t\tTrain-loss\t\tTest-loss\tTrain-IoU\tTest-IoU\t\tlearningRate\n",
    "        with open(automated_log_path, \"a\") as myfile:\n",
    "            myfile.write(\"\\n%d\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.4f\\t\\t%.8f\" % (epoch, average_epoch_loss_train, average_epoch_loss_val, iouTrain, iouVal, usedLr ))\n",
    "    \n",
    "    return(model)   #return model (convenience for encoder-decoder training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a3c6a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, is_best, filenameCheckpoint, filenameBest):\n",
    "    torch.save(state, filenameCheckpoint)\n",
    "    if is_best:\n",
    "        print (\"Saving model as best\")\n",
    "        torch.save(state, filenameBest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "83e2ad05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    savedir = f'{args.savedir}'\n",
    "\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "\n",
    "    with open(savedir + '/opts.txt', \"w\") as myfile:\n",
    "        myfile.write(str(args))\n",
    "\n",
    "    #Load Model\n",
    "    # assert os.path.exists(args.model + \".py\"), \"Error: model definition not found\"\n",
    "    # model_file = importlib.import_module(args.model)\n",
    "    model = Net(NUM_CLASSES)\n",
    "    # copyfile(args.model + \".py\", savedir + '/' + args.model + \".py\")\n",
    "    \n",
    "    if args.cuda:\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "    \n",
    "    if args.state:\n",
    "        #if args.state is provided then load this state for training\n",
    "        #Note: this only loads initialized weights. If you want to resume a training use \"--resume\" option!!\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model.load_state_dict(torch.load(args.state))\n",
    "        except AssertionError:\n",
    "            model.load_state_dict(torch.load(args.state,\n",
    "                map_location=lambda storage, loc: storage))\n",
    "        #When model is saved as DataParallel it adds a model. to each key. To remove:\n",
    "        #state_dict = {k.partition('model.')[2]: v for k,v in state_dict}\n",
    "        #https://discuss.pytorch.org/t/prefix-parameter-names-in-saved-model-if-trained-by-multi-gpu/494\n",
    "        \"\"\"\n",
    "        def load_my_state_dict(model, state_dict):  #custom function to load model when not all dict keys are there\n",
    "            own_state = model.state_dict()\n",
    "            for name, param in state_dict.items():\n",
    "                if name not in own_state:\n",
    "                     continue\n",
    "                own_state[name].copy_(param)\n",
    "            return model\n",
    "\n",
    "        #print(torch.load(args.state))\n",
    "        model = load_my_state_dict(model, torch.load(args.state))\n",
    "\n",
    "    \"\"\"\n",
    "    def weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if classname.find('Conv') != -1:\n",
    "            #m.weight.data.normal_(0.0, 0.02)\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "        elif classname.find('BatchNorm') != -1:\n",
    "            #m.weight.data.normal_(1.0, 0.02)\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.fill_(0)\n",
    "    #TO ACCESS MODEL IN DataParallel: next(model.children())\n",
    "    #next(model.children()).decoder.apply(weights_init)\n",
    "    #Reinitialize weights for decoder\n",
    "    \n",
    "    next(model.children()).decoder.layers.apply(weights_init)\n",
    "    next(model.children()).decoder.output_conv.apply(weights_init)\n",
    "    #print(model.state_dict())\n",
    "    f = open('weights5.txt', 'w')\n",
    "    f.write(str(model.state_dict()))\n",
    "    f.close()\n",
    "    \"\"\"\n",
    "\n",
    "    # train(args, model)\n",
    "    if (not args.decoder):\n",
    "        print(\"========== ENCODER TRAINING ===========\")\n",
    "        model = train(args, model, True) #Train encoder\n",
    "    #CAREFUL: for some reason, after training encoder alone, the decoder gets weights=0. \n",
    "    #We must reinit decoder weights or reload network passing only encoder in order to train decoder\n",
    "    print(\"========== DECODER TRAINING ===========\")\n",
    "    if (not args.state):\n",
    "        if args.pretrainedEncoder:\n",
    "            print(\"Loading encoder pretrained in imagenet\")\n",
    "            from erfnet_imagenet import ERFNet as ERFNet_imagenet\n",
    "            pretrainedEnc = torch.nn.DataParallel(ERFNet_imagenet(1000))\n",
    "            pretrainedEnc.load_state_dict(torch.load(args.pretrainedEncoder)['state_dict'])\n",
    "            pretrainedEnc = next(pretrainedEnc.children()).features.encoder\n",
    "            if (not args.cuda):\n",
    "                pretrainedEnc = pretrainedEnc.cpu()     #because loaded encoder is probably saved in cuda\n",
    "        else:\n",
    "            pretrainedEnc = next(model.children()).encoder\n",
    "        model = Net(NUM_CLASSES, encoder=pretrainedEnc)  #Add decoder to encoder\n",
    "        if args.cuda:\n",
    "            model = torch.nn.DataParallel(model).cuda()\n",
    "        #When loading encoder reinitialize weights for decoder because they are set to 0 when training dec\n",
    "    model = train(args, model, False)   #Train decoder\n",
    "    print(\"========== TRAINING FINISHED ===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1d311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e63f9c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
