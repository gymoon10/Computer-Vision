{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Modules.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvOmO36C-Suy"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Anchor Generator\n"
      ],
      "metadata": {
        "id": "W2XGQsrQD8zS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "oh74OMj_-UX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnchorGenerator:\n",
        "    '''이미지의 각 grid cell에 서로 다른 종횡비를 갖는 9개의 anchor box 정의'''\n",
        "    def __init__(self, sizes, ratios):\n",
        "        self.sizes = sizes  # anchor box의 w, h - [128, 256, 512]\n",
        "        self.ratios = ratios  # anchor box의 w, h 길이 비율 - [0.5, 1, 2]\n",
        "        \n",
        "        self.cell_anchor = None\n",
        "        self._cache = {}\n",
        "\n",
        "    def set_cell_anchor(self, dtype, device): \n",
        "        '''scale_sizes, aspect_ratios를 고려한 pre-defined anchor box 9개 정의'''\n",
        "        if self.cell_anchor is not None:\n",
        "            return \n",
        "\n",
        "        sizes = torch.tensor(self.sizes, dtype=dtype, device=device)  # tensor([128, 256, 512])\n",
        "        ratios = torch.tensor(self.ratios, dtype=dtype, device=device)  # tensor([0.5, 1, 2])\n",
        "\n",
        "        h_ratios = torch.sqrt(ratios)\n",
        "        w_ratios = 1 / h_ratios\n",
        "\n",
        "        hs = (sizes[:, None] * h_ratios[None, :]).view(-1)\n",
        "        ws = (sizes[:, None] * w_ratios[None, :]).view(-1)\n",
        "\n",
        "        self.cell_anchor = torch.stack([-ws, -hs, ws, hs], dim=1) / 2  # (9, 4) - 9개의 anchor box 정보\n",
        "\n",
        "    def grid_anchor(self, grid_size, stride):\n",
        "        '''각 grid cell마다 중심을 기준으로 9개의 anchor box 생성'''\n",
        "        dtype, device = self.cell_anchor.dtype, self.cell_anchor.device\n",
        "\n",
        "        # 각 grid cell의 서로 다른 중심 좌표 반영 (stride 이용)\n",
        "        shift_x = torch.arange(0, grid_size[1], dtype=dtype, device=device) * stride[1]\n",
        "        shift_y = torch.arange(0, grid_size[0], dtype=dtype, device=device) * stride[0]\n",
        "\n",
        "        y, x = torch.meshgrid(shift_y, shift_x)  # 중심 좌표 기준으로 격자\n",
        "        x = x.reshape(-1)\n",
        "        y = y.reshape(-1)\n",
        "        shift = torch.stack((x, y, x, y), dim=1).reshape(-1, 1, 4)\n",
        "\n",
        "        anchor = (shift + self.cell_anchor).reshape(-1, 4)\n",
        "\n",
        "        return anchor\n",
        "\n",
        "    def cached_grid_anchor(self, grid_size, stride):\n",
        "        key = grid_size + stride\n",
        "        if key in self._cache:\n",
        "            return self._cache[key]\n",
        "        anchor = self.grid_anchor(grid_size, stride)\n",
        "        \n",
        "        if len(self._cache) >= 3:\n",
        "            self._cache.clear()\n",
        "        self._cache[key] = anchor\n",
        "\n",
        "        return anchor\n",
        "\n",
        "    def __call__(self, feature, image_size):\n",
        "        dtype, device = feature.dtype, feature.device\n",
        "        grid_size = tuple(feature.shape[-2:])  # w, h\n",
        "        stride = tuple(int(i / g) for i, g in zip(image_size, grid_size))  # 각 grid cell간의 가로, 세로 간격\n",
        "        \n",
        "        self.set_cell_anchor(dtype, device)\n",
        "        \n",
        "        anchor = self.cached_grid_anchor(grid_size, stride)\n",
        "        \n",
        "        return anchor"
      ],
      "metadata": {
        "id": "16wz4wVV-VJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. RPN\n"
      ],
      "metadata": {
        "id": "mUfmJHjwED9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "9GdKOw-4EJ4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Utils"
      ],
      "metadata": {
        "id": "zgAiuP2qEjlj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Matcher:\n",
        "    def __init__(self, high_threshold, low_threshold, allow_low_quality_matches=False):\n",
        "        self.high_threshold = high_threshold\n",
        "        self.low_threshold = low_threshold\n",
        "        self.allow_low_quality_matches = allow_low_quality_matches\n",
        "    \n",
        "    def __call__(self, iou):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            iou (Tensor[M, N]): 각 M개의 gt box의 N개의 pred box에 대한 iou 값 (pairwise-quality)\n",
        "\n",
        "        Returns:\n",
        "            label (Tensor[N]): N개의 pred box에 대한 label 예측 (1, 0, -1)\n",
        "            matched_idx (Tensor[N]): N개의 pred box에 매칭되는 gt box의 index\n",
        "        \"\"\"\n",
        "        value, matched_idx = iou.max(dim=0)  # 행 기준 최댓값 - 각 gt box에 대해 가장 큰 iou 값을 갖는 pred box와 해당 iou값\n",
        "        label = torch.full((iou.shape[1],), -1, dtype=torch.float, device=iou.device)\n",
        "\n",
        "        # label 할당 (1: positive / 0: negative / -1: ignore)\n",
        "        label[value >= self.high_threshold] = 1\n",
        "        label[value < self.low_threshold] = 0\n",
        "\n",
        "        if self.allow_low_quality_matches:\n",
        "            highest_quality = iou.max(dim=1)[0]\n",
        "            gt_pred_pairs = torch.where(iou == highest_quality[:, None])[1]\n",
        "            label[gt_pred_pairs] = 1\n",
        "\n",
        "        return label, matched_idx"
      ],
      "metadata": {
        "id": "FIQ00qoE_UYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BalancedPositiveNegativeSampler:\n",
        "    '''Class imbalance 해결용 (negative label이 positive보다 훨씬 많기 때문에)'''\n",
        "    def __init__(self, num_samples, positive_fraction):\n",
        "        self.num_samples = num_samples  # 학습하는데 사용하는 anchor의 수\n",
        "        self.positive_fraction = positive_fraction  # 샘플링되는 anchor들 중 positive label을 갖는 anchor의 비율\n",
        "\n",
        "    def __call__(self, label):\n",
        "        positive = torch.where(label == 1)[0]\n",
        "        negative = torch.where(label == 0)[0]\n",
        "        \n",
        "        # 설정한 positive_fraction에 의해 positive/negative의 샘플링 수가 정의됨\n",
        "        num_pos = int(self.num_samples * self.positive_fraction)\n",
        "        num_pos = min(positive.numel(), num_pos)\n",
        "        num_neg = self.num_samples - num_pos\n",
        "        num_neg = min(negative.numel(), num_neg)\n",
        "\n",
        "        pos_perm = torch.randperm(positive.numel(), device=positive.device)[:num_pos]\n",
        "        neg_perm = torch.randperm(negative.numel(), device=negative.device)[:num_neg]\n",
        "\n",
        "        pos_idx = positive[pos_perm]\n",
        "        neg_idx = negative[neg_perm]\n",
        "\n",
        "        return pos_idx, neg_idx"
      ],
      "metadata": {
        "id": "58sa_DfaKMCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "class BoxCoder:\n",
        "    def __init__(self, weights, bbox_xform_clip=math.log(1000. / 16)):\n",
        "        self.weights = weights\n",
        "        self.bbox_xform_clip = bbox_xform_clip\n",
        "\n",
        "    def encode(self, reference_box, proposal):\n",
        "        \"\"\"\n",
        "        Encode a set of proposals with respect to some\n",
        "        reference boxes\n",
        "        Arguments:\n",
        "            reference_boxes (Tensor[N, 4]): reference boxes\n",
        "            proposals (Tensor[N, 4]): boxes to be encoded\n",
        "        \"\"\"\n",
        "        \n",
        "        width = proposal[:, 2] - proposal[:, 0]\n",
        "        height = proposal[:, 3] - proposal[:, 1]\n",
        "        ctr_x = proposal[:, 0] + 0.5 * width\n",
        "        ctr_y = proposal[:, 1] + 0.5 * height\n",
        "\n",
        "        gt_width = reference_box[:, 2] - reference_box[:, 0]\n",
        "        gt_height = reference_box[:, 3] - reference_box[:, 1]\n",
        "        gt_ctr_x = reference_box[:, 0] + 0.5 * gt_width\n",
        "        gt_ctr_y = reference_box[:, 1] + 0.5 * gt_height\n",
        "\n",
        "        dx = self.weights[0] * (gt_ctr_x - ctr_x) / width\n",
        "        dy = self.weights[1] * (gt_ctr_y - ctr_y) / height\n",
        "        dw = self.weights[2] * torch.log(gt_width / width)\n",
        "        dh = self.weights[3] * torch.log(gt_height / height)\n",
        "\n",
        "        delta = torch.stack((dx, dy, dw, dh), dim=1)\n",
        "        return delta\n",
        "\n",
        "    def decode(self, delta, box):\n",
        "        \"\"\"\n",
        "        From a set of original boxes and encoded relative box offsets,\n",
        "        get the decoded boxes.\n",
        "        Arguments:\n",
        "            delta (Tensor[N, 4]): encoded boxes.\n",
        "            boxes (Tensor[N, 4]): reference boxes.\n",
        "        \"\"\"  \n",
        "        dx = delta[:, 0] / self.weights[0]\n",
        "        dy = delta[:, 1] / self.weights[1]\n",
        "        dw = delta[:, 2] / self.weights[2]\n",
        "        dh = delta[:, 3] / self.weights[3]\n",
        "\n",
        "        dw = torch.clamp(dw, max=self.bbox_xform_clip)\n",
        "        dh = torch.clamp(dh, max=self.bbox_xform_clip)\n",
        "\n",
        "        width = box[:, 2] - box[:, 0]\n",
        "        height = box[:, 3] - box[:, 1]\n",
        "        ctr_x = box[:, 0] + 0.5 * width\n",
        "        ctr_y = box[:, 1] + 0.5 * height\n",
        "\n",
        "        pred_ctr_x = dx * width + ctr_x\n",
        "        pred_ctr_y = dy * height + ctr_y\n",
        "        pred_w = torch.exp(dw) * width\n",
        "        pred_h = torch.exp(dh) * height\n",
        "\n",
        "        xmin = pred_ctr_x - 0.5 * pred_w\n",
        "        ymin = pred_ctr_y - 0.5 * pred_h\n",
        "        xmax = pred_ctr_x + 0.5 * pred_w\n",
        "        ymax = pred_ctr_y + 0.5 * pred_h\n",
        "\n",
        "        target = torch.stack((xmin, ymin, xmax, ymax), dim=1)\n",
        "        return target"
      ],
      "metadata": {
        "id": "P40orOUJLvO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_box(box, score, image_shape, min_size):\n",
        "    \"\"\"\n",
        "    Clip boxes in the image size and remove boxes which are too small.\n",
        "    \"\"\"\n",
        "    \n",
        "    box[:, [0, 2]] = box[:, [0, 2]].clamp(0, image_shape[1]) \n",
        "    box[:, [1, 3]] = box[:, [1, 3]].clamp(0, image_shape[0]) \n",
        "\n",
        "    w, h = box[:, 2] - box[:, 0], box[:, 3] - box[:, 1]\n",
        "    keep = torch.where((w >= min_size) & (h >= min_size))[0]\n",
        "    box, score = box[keep], score[keep]\n",
        "    return box, score"
      ],
      "metadata": {
        "id": "1GKAO5zbNooN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nms(box, score, threshold):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        box (Tensor[N, 4])\n",
        "        score (Tensor[N]): scores of the boxes.\n",
        "        threshold (float): iou threshold.\n",
        "    Returns: \n",
        "        keep (Tensor): indices of boxes filtered by NMS.\n",
        "    \"\"\"\n",
        "    \n",
        "    return torch.ops.torchvision.nms(box, score, threshold)"
      ],
      "metadata": {
        "id": "yoKNAJYNNsfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def box_iou(box_a, box_b):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        boxe_a (Tensor[N, 4])\n",
        "        boxe_b (Tensor[M, 4])\n",
        "    Returns:\n",
        "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
        "            IoU values for every element in box_a and box_b\n",
        "    \"\"\"\n",
        "    \n",
        "    lt = torch.max(box_a[:, None, :2], box_b[:, :2])\n",
        "    rb = torch.min(box_a[:, None, 2:], box_b[:, 2:])\n",
        "\n",
        "    wh = (rb - lt).clamp(min=0)\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]\n",
        "    area_a = torch.prod(box_a[:, 2:] - box_a[:, :2], 1)\n",
        "    area_b = torch.prod(box_b[:, 2:] - box_b[:, :2], 1)\n",
        "    \n",
        "    return inter / (area_a[:, None] + area_b - inter)"
      ],
      "metadata": {
        "id": "6WQ1IlcVN9A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 RPN Head"
      ],
      "metadata": {
        "id": "y8Oer5oaL8vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RPNHead(nn.Module):\n",
        "    def __init__(self, in_channels, num_anchors):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, in_channels, 3, 1, 1)\n",
        "        self.cls_logits = nn.Conv2d(in_channels, num_anchors, 1)  # class label 예측\n",
        "        self.bbox_pred = nn.Conv2d(in_channels, 4 * num_anchors, 1)  # bbox 예측\n",
        "        \n",
        "        for l in self.children():\n",
        "            nn.init.normal_(l.weight, std=0.01)\n",
        "            nn.init.constant_(l.bias, 0)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv(x))\n",
        "        logits = self.cls_logits(x)\n",
        "        bbox_reg = self.bbox_pred(x)\n",
        "\n",
        "        return logits, bbox_reg  # objectness, pred_bbox_delta"
      ],
      "metadata": {
        "id": "ZZGuPAGGL-vs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 RPN Network"
      ],
      "metadata": {
        "id": "6FgYC1R0Lh4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RegionProposalNetwork(nn.Module):\n",
        "    def __init__(self, anchor_generator, head, \n",
        "                 fg_iou_thresh, bg_iou_thresh,\n",
        "                 num_samples, positive_fraction,\n",
        "                 reg_weights,\n",
        "                 pre_nms_top_n, post_nms_top_n, nms_thresh):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.anchor_generator = anchor_generator  # anchor generator layer\n",
        "        self.head = head  # head(prediction) layer\n",
        "        \n",
        "        # utils\n",
        "        self.proposal_matcher = Matcher(fg_iou_thresh, bg_iou_thresh, allow_low_quality_matches=True)\n",
        "        self.fg_bg_sampler = BalancedPositiveNegativeSampler(num_samples, positive_fraction)\n",
        "        self.box_coder = BoxCoder(reg_weights)  # bbox offset 예측을 반영하여 좌표 변환\n",
        "        \n",
        "        self._pre_nms_top_n = pre_nms_top_n  # NMS 적용 전에 남겨둘 proposal 수\n",
        "        self._post_nms_top_n = post_nms_top_n  # NMS 적용 후에 남겨둘 proposal 수\n",
        "        self.nms_thresh = nms_thresh  # RPN proposal에 대한 post-processing의 ths  \n",
        "        self.min_size = 1\n",
        "\n",
        "    def create_proposal(self, anchor, objectness, pred_bbox_delta, image_shape):\n",
        "        if self.training:\n",
        "            pre_nms_top_n = self._pre_nms_top_n['training']\n",
        "            post_nms_top_n = self._post_nms_top_n['training']\n",
        "        else:\n",
        "            pre_nms_top_n = self._pre_nms_top_n['testing']\n",
        "            post_nms_top_n = self._post_nms_top_n['testing']             \n",
        "\n",
        "        pre_nms_top_n = min(objectness.shape[0], pre_nms_top_n)\n",
        "        top_n_idx = objectness.topk(pre_nms_top_n)[1]  # 산출된 bbox들 중 확률이 높은 n개 사용\n",
        "\n",
        "        score = objectness[top_n_idx]  \n",
        "        proposal = self.box_coder.decode(pred_bbox_delta[top_n_idx], anchor[top_n_idx])  # 상위 n개의 proposal\n",
        "\n",
        "        proposal, score = process_box(proposal, score, image_shape, self.min_size)\n",
        "        keep = nms(proposal, score, self.nms_thresh)[:post_nms_top_n]  # 최적의 proposal만 추출\n",
        "        proposal = proposal[keep]\n",
        "\n",
        "        return proposal\n",
        "\n",
        "    def compute_loss(self, objectness, pred_bbox_delta, gt_box, anchor):\n",
        "        iou = box_iou(gt_box, anchor)  # iou 계산\n",
        "        label, matched_idx = self.proposal_matcher(iou)  # 각 pred box의 label 예측, pred box에 대응되는 gt box의 idx \n",
        "        \n",
        "        # box regression target\n",
        "        pos_idx, neg_idx = self.fg_bg_sampler(label)  # class 비율을 맞춰서 샘플링\n",
        "        idx = torch.cat((pos_idx, neg_idx))\n",
        "        regression_target = self.box_coder.encode(gt_box[matched_idx[pos_idx]], anchor[pos_idx])\n",
        "        \n",
        "        # loss\n",
        "        objectness_loss = F.binary_cross_entropy_with_logits(objectness[idx], label[idx])\n",
        "        box_loss = F.l1_loss(pred_bbox_delta[pos_idx], regression_target, reduction='sum') / idx.numel()\n",
        "\n",
        "        return objectness_loss, box_loss\n",
        "\n",
        "    def forward(self, feature, image_shape, target=None):\n",
        "        if target is not None:\n",
        "            gt_box = target['boxes']\n",
        "        anchor = self.anchor_generator(feature, image_shape)\n",
        "        \n",
        "        objectness, pred_bbox_delta = self.head(feature)  # feature를 입력으로 받아 detection 예측 수행\n",
        "        objectness = objectness.permute(0, 2, 3, 1).flatten()\n",
        "        pred_bbox_delta = pred_bbox_delta.permute(0, 2, 3, 1).reshape(-1, 4)\n",
        "\n",
        "        proposal = self.create_proposal(anchor, objectness.detach(), pred_bbox_delta.detach(), image_shape)  # 최적의 region proposals (bbox 좌표 형식)\n",
        "        if self.training:  # 학습시 GT, 최적의 proposal간의 loss 계산\n",
        "            objectness_loss, box_loss = self.compute_loss(objectness, pred_bbox_delta, gt_box, anchor)\n",
        "            return proposal, dict(rpn_objectness_loss=objectness_loss, rpn_box_loss=box_loss)\n",
        "        \n",
        "        return proposal, {}"
      ],
      "metadata": {
        "id": "_HRCC7zBKO9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. RoI Heads"
      ],
      "metadata": {
        "id": "pgUbQGAdQjcR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 RoI Align"
      ],
      "metadata": {
        "id": "tn_AWZKKQvnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch"
      ],
      "metadata": {
        "id": "LisliBpTRLpF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://pytorch.org/vision/stable/_modules/torchvision/ops/roi_align.html\n",
        "def roi_align(features, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio):\n",
        "    if torch.__version__ >= \"1.5.0\":\n",
        "        return torch.ops.torchvision.roi_align(\n",
        "            features, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio, False)  # pooled_h x pooled_w 크기의 align 출력 반환\n",
        "        \n",
        "    else:\n",
        "        return torch.ops.torchvision.roi_align(\n",
        "            features, rois, spatial_scale, pooled_height, pooled_width, sampling_ratio)"
      ],
      "metadata": {
        "id": "ZIDPzZ9MRbeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RoIAlign:\n",
        "    def __init__(self, output_size, sampling_ratio):\n",
        "        self.output_size = output_size  # 고정된 크기의 RoIAlign 출력\n",
        "        self.sampling_ratio = sampling_ratio\n",
        "        self.spatial_scale = None\n",
        "        \n",
        "    def setup_scale(self, feature_shape, image_shape):\n",
        "        if self.spatial_scale is not None:\n",
        "            return\n",
        "        \n",
        "        possible_scales = []\n",
        "        for s1, s2 in zip(feature_shape, image_shape):\n",
        "            scale = 2 ** int(math.log2(s1 / s2))\n",
        "            possible_scales.append(scale)\n",
        "        assert possible_scales[0] == possible_scales[1]\n",
        "        self.spatial_scale = possible_scales[0]\n",
        "        \n",
        "    def __call__(self, feature, proposal, image_shape):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            feature (Tensor[N, C, H, W])\n",
        "            proposal (Tensor[K, 4])\n",
        "            image_shape (Torch.Size([H, W]))\n",
        "\n",
        "        Returns:\n",
        "            output (Tensor[K, C, self.output_size[0], self.output_size[1]])\n",
        "        \"\"\"\n",
        "        idx = proposal.new_full((proposal.shape[0], 1), 0)\n",
        "        roi = torch.cat((idx, proposal), dim=1)\n",
        "        \n",
        "        self.setup_scale(feature.shape[-2:], image_shape)\n",
        "        \n",
        "        # feature map에서 RoI bbox 좌표에 해당하는 영역에 대해 align 연산 수행\n",
        "        return roi_align(feature.to(roi), roi, self.spatial_scale, self.output_size[0], self.output_size[1], self.sampling_ratio)"
      ],
      "metadata": {
        "id": "ZCieqfIqOVS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Predictiors"
      ],
      "metadata": {
        "id": "EPca0c2rSRfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import OrderedDict"
      ],
      "metadata": {
        "id": "MMxmb7Ju5_SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FastRCNNPredictor(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_channels, mid_channels)\n",
        "        self.fc2 = nn.Linear(mid_channels, mid_channels)\n",
        "        self.cls_score = nn.Linear(mid_channels, num_classes)\n",
        "        self.bbox_pred = nn.Linear(mid_channels, num_classes * 4)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x.flatten(start_dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "\n",
        "        score = self.cls_score(x)\n",
        "        bbox_delta = self.bbox_pred(x)\n",
        "\n",
        "        return score, bbox_delta        "
      ],
      "metadata": {
        "id": "_-f6sdgsRk_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskRCNNPredictor(nn.Sequential):\n",
        "    def __init__(self, in_channels, layers, dim_reduced, num_classes):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            in_channels (int)\n",
        "            layers (Tuple[int])\n",
        "            dim_reduced (int)\n",
        "            num_classes (int)\n",
        "        \"\"\"\n",
        "        \n",
        "        d = OrderedDict()\n",
        "        next_feature = in_channels\n",
        "        # layers = [256, 256, 256, 256]\n",
        "        for layer_idx, layer_features in enumerate(layers, 1):\n",
        "            d['mask_fcn{}'.format(layer_idx)] = nn.Conv2d(next_feature, layer_features, 3, 1, 1)\n",
        "            d['relu{}'.format(layer_idx)] = nn.ReLU(inplace=True)\n",
        "            next_feature = layer_features\n",
        "        \n",
        "        d['mask_conv5'] = nn.ConvTranspose2d(next_feature, dim_reduced, 2, 2, 0)\n",
        "        d['relu5'] = nn.ReLU(inplace=True)\n",
        "        d['mask_fcn_logits'] = nn.Conv2d(dim_reduced, num_classes, 1, 1, 0)  # 256, 21\n",
        "        super().__init__(d)\n",
        "\n",
        "        for name, param in self.named_parameters():\n",
        "            if 'weight' in name:\n",
        "                nn.init.kaiming_normal_(param, mode='fan_out', nonlinearity='relu')"
      ],
      "metadata": {
        "id": "up59NXcZ56sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "uupmektdS4S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Multi task loss\n",
        "\n",
        "2-stage loss\n",
        "\n",
        "L = L_cls + L_box + L_mask\n",
        "\n"
      ],
      "metadata": {
        "id": "qi_Mhp1pxTBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fastrcnn_loss(class_logit, box_regression, label, regression_target):\n",
        "    classifier_loss = F.cross_entropy(class_logit, label)\n",
        "\n",
        "    N, num_pos = class_logit.shape[0], regression_target.shape[0]\n",
        "    box_regression = box_regression.reshape(N, -1, 4)\n",
        "    box_regression, label = box_regression[:num_pos], label[:num_pos]\n",
        "    box_idx = torch.arange(num_pos, device=label.device)\n",
        "\n",
        "    box_reg_loss = F.smooth_l1_loss(box_regression[box_idx, label], regression_target, reduction='sum') / N\n",
        "\n",
        "    return classifier_loss, box_reg_loss  # L_cls, L_box\n",
        "\n",
        "\n",
        "def maskrcnn_loss(mask_logit, proposal, matched_idx, label, gt_mask):\n",
        "    matched_idx = matched_idx[:, None].to(proposal)\n",
        "    roi = torch.cat((matched_idx, proposal), dim=1)\n",
        "            \n",
        "    M = mask_logit.shape[-1]\n",
        "    gt_mask = gt_mask[:, None].to(roi)\n",
        "    mask_target = roi_align(gt_mask, roi, 1., M, M, -1)[:, 0]\n",
        "\n",
        "    idx = torch.arange(label.shape[0], device=label.device)\n",
        "    mask_loss = F.binary_cross_entropy_with_logits(mask_logit[idx, label], mask_target)\n",
        "\n",
        "    return mask_loss  # L_mask"
      ],
      "metadata": {
        "id": "tlYcvClaS-D_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 RoI Heads"
      ],
      "metadata": {
        "id": "x2UQxB1k74u5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RoIHeads(nn.Module):\n",
        "    def __init__(self, box_roi_pool, box_predictor,\n",
        "                 fg_iou_thresh, bg_iou_thresh,\n",
        "                 num_samples, positive_fraction,\n",
        "                 reg_weights,\n",
        "                 score_thresh, nms_thresh, num_detections):\n",
        "        super().__init__()\n",
        "        self.box_roi_pool = box_roi_pool\n",
        "        self.box_predictor = box_predictor\n",
        "        \n",
        "        self.mask_roi_pool = None\n",
        "        self.mask_predictor = None\n",
        "        \n",
        "        # utils\n",
        "        self.proposal_matcher = Matcher(fg_iou_thresh, bg_iou_thresh, allow_low_quality_matches=False)\n",
        "        self.fg_bg_sampler = BalancedPositiveNegativeSampler(num_samples, positive_fraction)\n",
        "        self.box_coder = BoxCoder(reg_weights)\n",
        "        \n",
        "        self.score_thresh = score_thresh\n",
        "        self.nms_thresh = nms_thresh\n",
        "        self.num_detections = num_detections\n",
        "        self.min_size = 1\n",
        "        \n",
        "    def has_mask(self):\n",
        "        if self.mask_roi_pool is None:\n",
        "            return False\n",
        "        if self.mask_predictor is None:\n",
        "            return False\n",
        "        return True\n",
        "        \n",
        "    def select_training_samples(self, proposal, target):\n",
        "        # Fast R-CNN GTs\n",
        "        gt_box = target['boxes']\n",
        "        gt_label = target['labels']\n",
        "\n",
        "        # Regional Proposals\n",
        "        proposal = torch.cat((proposal, gt_box))\n",
        "        \n",
        "        iou = box_iou(gt_box, proposal)  # iou 계산\n",
        "        # 예측 proposal bbox들의 class(-1, 0, 1), 각 GT box에 매칭되는(최대의 iou를 갖는) 예측 box의 index 반환 \n",
        "        pos_neg_label, matched_idx = self.proposal_matcher(iou)  \n",
        "        pos_idx, neg_idx = self.fg_bg_sampler(pos_neg_label)  # pos/neg의 비율을 적절히 맞춰서 샘플링\n",
        "        idx = torch.cat((pos_idx, neg_idx))\n",
        "        \n",
        "        regression_target = self.box_coder.encode(gt_box[matched_idx[pos_idx]], proposal[pos_idx])  # GT-예측 pair\n",
        "        proposal = proposal[idx]\n",
        "        matched_idx = matched_idx[idx]\n",
        "        label = gt_label[matched_idx]\n",
        "        num_pos = pos_idx.shape[0]\n",
        "        label[num_pos:] = 0\n",
        "        \n",
        "        return proposal, matched_idx, label, regression_target\n",
        "    \n",
        "    def fastrcnn_inference(self, class_logit, box_regression, proposal, image_shape):\n",
        "        N, num_classes = class_logit.shape\n",
        "        \n",
        "        device = class_logit.device\n",
        "        pred_score = F.softmax(class_logit, dim=-1)\n",
        "        box_regression = box_regression.reshape(N, -1, 4)\n",
        "        \n",
        "        boxes = []\n",
        "        labels = []\n",
        "        scores = []\n",
        "        for l in range(1, num_classes):\n",
        "            score, box_delta = pred_score[:, l], box_regression[:, l]\n",
        "\n",
        "            keep = score >= self.score_thresh  # 특정 ths 이상의 bbox들의 idx\n",
        "            box, score, box_delta = proposal[keep], score[keep], box_delta[keep]\n",
        "            box = self.box_coder.decode(box_delta, box)\n",
        "            \n",
        "            box, score = process_box(box, score, image_shape, self.min_size)  # post-processing\n",
        "            \n",
        "            keep = nms(box, score, self.nms_thresh)[:self.num_detections]  # NMS\n",
        "            box, score = box[keep], score[keep]\n",
        "            label = torch.full((len(keep),), l, dtype=keep.dtype, device=device)\n",
        "            \n",
        "            boxes.append(box)  # bbox for detected objects\n",
        "            labels.append(label)  # classification for detected objects\n",
        "            scores.append(score)  # objectness score for detected objects\n",
        "\n",
        "        results = dict(boxes=torch.cat(boxes), labels=torch.cat(labels), scores=torch.cat(scores))\n",
        "        return results\n",
        "    \n",
        "    def forward(self, feature, proposal, image_shape, target):\n",
        "        '''feature : backbone feature map output\n",
        "           proposal : RPN output'''\n",
        "        if self.training:\n",
        "            proposal, matched_idx, label, regression_target = self.select_training_samples(proposal, target)\n",
        "        \n",
        "        # Detection\n",
        "        # RoI Align을 통해 feature map에서 proposal bbox에 해당되는 영역의 feature계산\n",
        "        box_feature = self.box_roi_pool(feature, proposal, image_shape)  # RoI Align\n",
        "        class_logit, box_regression = self.box_predictor(box_feature)  # Fast R-CNN predictor\n",
        "        \n",
        "        result, losses = {}, {}\n",
        "        if self.training:\n",
        "            # L_cls, L_box\n",
        "            classifier_loss, box_reg_loss = fastrcnn_loss(class_logit, box_regression, label, regression_target)\n",
        "            losses = dict(roi_classifier_loss=classifier_loss, roi_box_loss=box_reg_loss)\n",
        "\n",
        "        else:\n",
        "            result = self.fastrcnn_inference(class_logit, box_regression, proposal, image_shape)\n",
        "\n",
        "        # Mask pred    \n",
        "        if self.has_mask():\n",
        "            if self.training:\n",
        "                num_pos = regression_target.shape[0]\n",
        "\n",
        "                # proposal중 pos label에 해당되는 것들만 (검출된 object에 대해 mask prediction 수행) \n",
        "                mask_proposal = proposal[:num_pos] \n",
        "                pos_matched_idx = matched_idx[:num_pos]\n",
        "                mask_label = label[:num_pos]\n",
        "                \n",
        "                '''\n",
        "                # -------------- critial ----------------\n",
        "                box_regression = box_regression[:num_pos].reshape(num_pos, -1, 4)\n",
        "                idx = torch.arange(num_pos, device=mask_label.device)\n",
        "                mask_proposal = self.box_coder.decode(box_regression[idx, mask_label], mask_proposal)\n",
        "                # ---------------------------------------\n",
        "                '''\n",
        "                \n",
        "                if mask_proposal.shape[0] == 0:\n",
        "                    losses.update(dict(roi_mask_loss=torch.tensor(0)))\n",
        "                    return result, losses\n",
        "            else:\n",
        "                mask_proposal = result['boxes']\n",
        "                \n",
        "                if mask_proposal.shape[0] == 0:\n",
        "                    result.update(dict(masks=torch.empty((0, 28, 28))))\n",
        "                    return result, losses\n",
        "                \n",
        "            mask_feature = self.mask_roi_pool(feature, mask_proposal, image_shape)  # RoI Align\n",
        "            mask_logit = self.mask_predictor(mask_feature)  # Mask R-CNN predictor\n",
        "            \n",
        "            if self.training:\n",
        "                # L_mask\n",
        "                gt_mask = target['masks']\n",
        "                mask_loss = maskrcnn_loss(mask_logit, mask_proposal, pos_matched_idx, mask_label, gt_mask)\n",
        "                losses.update(dict(roi_mask_loss=mask_loss))\n",
        "            else:\n",
        "                label = result['labels']\n",
        "                idx = torch.arange(label.shape[0], device=label.device)\n",
        "                mask_logit = mask_logit[idx, label]\n",
        "\n",
        "                mask_prob = mask_logit.sigmoid()\n",
        "                result.update(dict(masks=mask_prob))\n",
        "                \n",
        "        return result, losses"
      ],
      "metadata": {
        "id": "ggR7BEhITRJA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}